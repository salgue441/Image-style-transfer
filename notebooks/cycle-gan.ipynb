{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 09:55:15.857744: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-14 09:55:16.000853: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731599716.052055    3300 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731599716.065931    3300 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-14 09:55:16.184709: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CycleGAN\n",
    "\n",
    "### Overview\n",
    "\n",
    "This implementation provides a complete TensorFlow-based CycleGAN framework for image-to-image translation. The codebase includes data processing, model architecture, training pipeline, and evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731599719.789161    3300 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpu_devices:\n",
    "    for device in gpu_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n",
    "\n",
    "else:\n",
    "    print(\"No GPU devices found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    height: int = 256\n",
    "    width: int = 256\n",
    "    channels: int = 3\n",
    "    base_filters: int = 64\n",
    "    lambda_cycle: float = 10.0\n",
    "    lambda_identity: float = 0.5\n",
    "    learning_rate: float = 2e-4\n",
    "    beta_1: float = 0.5\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"height\": self.height,\n",
    "            \"width\": self.width,\n",
    "            \"channels\": self.channels,\n",
    "            \"base_filters\": self.base_filters,\n",
    "            \"lambda_cycle\": self.lambda_cycle,\n",
    "            \"lambda_identity\": self.lambda_identity,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"beta_1\": self.beta_1,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProcessor:\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def decode_image(self, image: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Decodes and normalizes the image.\n",
    "\n",
    "        Args:\n",
    "          image: A tensor representing an image.\n",
    "\n",
    "        Returns:\n",
    "          The decoded and normalized image.\n",
    "        \"\"\"\n",
    "\n",
    "        image = tf.image.decode_jpeg(image, channels=self.config.channels)\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image = (image / 127.5) - 1\n",
    "\n",
    "        return image\n",
    "\n",
    "    @tf.function\n",
    "    def parse_tfrecord(self, example_photo):\n",
    "        \"\"\"\n",
    "        Parses a TFRecord example containing a photo.\n",
    "\n",
    "        Args:\n",
    "          example_photo: A TFRecord example containing a photo.\n",
    "\n",
    "        Returns:\n",
    "          The decoded and normalized photo.\n",
    "        \"\"\"\n",
    "\n",
    "        feature_description = {\n",
    "            \"image_name\": tf.io.FixedLenFeature([], tf.string),\n",
    "            \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "            \"target\": tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "\n",
    "        example = tf.io.parse_single_example(example_photo, feature_description)\n",
    "        return self.decode_image(example[\"image\"])\n",
    "\n",
    "    def create_dataset(\n",
    "        self,\n",
    "        filenames: list,\n",
    "        batch_size: int = 1,\n",
    "        shuffle: bool = True,\n",
    "        cache: bool = True,\n",
    "    ) -> tf.data.Dataset:\n",
    "        \"\"\"\n",
    "        Creates a dataset from TFRecord files.\n",
    "\n",
    "        Args:\n",
    "            filenames (list): A list of TFRecord filenames.\n",
    "            batch_size (int): The batch size.\n",
    "            shuffle (bool): Whether to shuffle the dataset.\n",
    "            cache (bool): Whether to cache the dataset in memory.\n",
    "\n",
    "        Returns:\n",
    "            A tf.data.Dataset instance.\n",
    "        \"\"\"\n",
    "\n",
    "        dataset = tf.data.TFRecordDataset(\n",
    "            filenames, num_parallel_reads=tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "\n",
    "        dataset = dataset.map(\n",
    "            self.parse_tfrecord, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=2048, reshuffle_each_iteration=True)\n",
    "\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "        if cache:\n",
    "            dataset = dataset.cache()\n",
    "\n",
    "        return dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualizationUtils:\n",
    "    @staticmethod\n",
    "    def denormalize_image(image: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Denormalizes the image by scaling it back to the [0, 255] range.\n",
    "\n",
    "        Args:\n",
    "            image: A tensor representing an image.\n",
    "\n",
    "        Returns:\n",
    "            The denormalized image.\n",
    "        \"\"\"\n",
    "\n",
    "        return (image * 0.5) + 0.5\n",
    "\n",
    "    @staticmethod\n",
    "    def create_figure(\n",
    "        nrows: int, ncols: int, figsize: Optional[Tuple[int, int]] = None\n",
    "    ) -> Tuple[plt.Figure, plt.Axes]:\n",
    "        \"\"\"\n",
    "        Create a Matplotlib figure and axes with the given number of rows and columns.\n",
    "\n",
    "        Args:\n",
    "            nrows (int): The number of rows.\n",
    "            ncols (int): The number of columns.\n",
    "            figsize (tuple): The figure size.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the figure and axes.\n",
    "        \"\"\"\n",
    "\n",
    "        if figsize is None:\n",
    "            figsize = (ncols * 4, nrows * 4)\n",
    "\n",
    "        fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "\n",
    "        if nrows * ncols == 1:\n",
    "            axes = np.array([axes])\n",
    "\n",
    "        return fig, axes.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"CycleGAN\", name=\"DiscriminatorBlock\")\n",
    "class DownsampleBlock(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filters: int,\n",
    "        size: int = 4,\n",
    "        strides: int = 2,\n",
    "        apply_norm: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.conv = tf.keras.layers.Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=size,\n",
    "            strides=strides,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "                mean=0.0, stddev=0.02\n",
    "            ),\n",
    "            use_bias=not apply_norm,\n",
    "        )\n",
    "\n",
    "        self.batch_norm = tf.keras.layers.BatchNormalization(\n",
    "            gamma_initializer=(\n",
    "                tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "                if apply_norm\n",
    "                else None\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.activation = tf.keras.layers.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "    def call(self, x: tf.Tensor, training: bool = True) -> tf.Tensor:\n",
    "        x = self.conv(x)\n",
    "        if self.batch_norm is not None:\n",
    "            x = self.batch_norm(x, training=training)\n",
    "\n",
    "        return self.activation(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"filters\": self.filters,\n",
    "                \"size\": self.size,\n",
    "                \"strides\": self.strides,\n",
    "                \"apply_norm\": self.apply_norm,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"CycleGAN\", name=\"UpsampleBlock\")\n",
    "class UpsampleBlock(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filters: int,\n",
    "        size: int = 4,\n",
    "        strides: int = 2,\n",
    "        apply_dropout: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.conv_transpose = tf.keras.layers.Conv2DTranspose(\n",
    "            filters=filters,\n",
    "            kernel_size=size,\n",
    "            strides=strides,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "                mean=0.0, stddev=0.02\n",
    "            ),\n",
    "            use_bias=False,\n",
    "        )\n",
    "\n",
    "        self.batch_norm = tf.keras.layers.BatchNormalization(\n",
    "            gamma_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "        )\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5) if apply_dropout else None\n",
    "        self.activation = tf.keras.layers.ReLU()\n",
    "\n",
    "    def call(self, x: tf.Tensor, training: bool = True) -> tf.Tensor:\n",
    "        x = self.conv_transpose(x)\n",
    "        x = self.batch_norm(x, training=training)\n",
    "\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x, training=training)\n",
    "\n",
    "        return self.activation(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"filters\": self.filters,\n",
    "                \"size\": self.size,\n",
    "                \"strides\": self.strides,\n",
    "                \"apply_dropout\": self.apply_dropout,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"CycleGAN\", name=\"Generator\")\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, config: ModelConfig, name: str = \"generator\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        if isinstance(config, dict):\n",
    "            config = ModelConfig(**config)\n",
    "\n",
    "        self.config = config\n",
    "        self.model_config = config\n",
    "\n",
    "        self.downsample_stack = [\n",
    "            DownsampleBlock(64, 4, apply_norm=False),\n",
    "            DownsampleBlock(128, 4),\n",
    "            DownsampleBlock(256, 4),\n",
    "            DownsampleBlock(512, 4),\n",
    "            DownsampleBlock(512, 4),\n",
    "            DownsampleBlock(512, 4),\n",
    "            DownsampleBlock(512, 4),\n",
    "            DownsampleBlock(512, 4),\n",
    "        ]\n",
    "\n",
    "        self.upsample_stack = [\n",
    "            UpsampleBlock(512, 4, apply_dropout=True),\n",
    "            UpsampleBlock(512, 4, apply_dropout=True),\n",
    "            UpsampleBlock(512, 4, apply_dropout=True),\n",
    "            UpsampleBlock(512, 4),\n",
    "            UpsampleBlock(256, 4),\n",
    "            UpsampleBlock(128, 4),\n",
    "            UpsampleBlock(64, 4),\n",
    "        ]\n",
    "\n",
    "        self.final_conv = tf.keras.layers.Conv2DTranspose(\n",
    "            filters=config.channels,\n",
    "            kernel_size=4,\n",
    "            strides=2,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "                mean=0.0, stddev=0.02\n",
    "            ),\n",
    "            activation=\"tanh\",\n",
    "        )\n",
    "\n",
    "    def call(self, x: tf.Tensor, training: bool = False) -> tf.Tensor:\n",
    "        skips = []\n",
    "        for down in self.downsample_stack:\n",
    "            x = down(x, training=training)\n",
    "            skips.append(x)\n",
    "\n",
    "        skips = reversed(skips[:-1])\n",
    "\n",
    "        for up, skip in zip(self.upsample_stack, skips):\n",
    "            x = up(x, training=training)\n",
    "            x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"config\": self.config.get_config(), \"name\": self.name}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(\n",
    "            config=ModelConfig.from_config(config[\"config\"]), name=config[\"name\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"CycleGAN\", name=\"Discriminator\")\n",
    "class Discriminator(tf.keras.Model):\n",
    "    \"\"\"Modern implementation of PatchGAN discriminator.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ModelConfig, name: str = \"discriminator\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.config = config\n",
    "\n",
    "        self.down_stack = [\n",
    "            DownsampleBlock(config.base_filters, apply_norm=False),\n",
    "            DownsampleBlock(config.base_filters * 2),\n",
    "            DownsampleBlock(config.base_filters * 4),\n",
    "        ]\n",
    "\n",
    "        self.zero_pad1 = tf.keras.layers.ZeroPadding2D()\n",
    "        self.conv = tf.keras.layers.Conv2D(\n",
    "            config.base_filters * 8,\n",
    "            4,\n",
    "            strides=1,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(0.0, 0.02),\n",
    "            use_bias=False,\n",
    "        )\n",
    "        self.batch_norm = tf.keras.layers.BatchNormalization(\n",
    "            gamma_initializer=tf.keras.initializers.RandomNormal(0.0, 0.02)\n",
    "        )\n",
    "        self.leaky_relu = tf.keras.layers.LeakyReLU(0.2)\n",
    "        self.zero_pad2 = tf.keras.layers.ZeroPadding2D()\n",
    "        self.final_conv = tf.keras.layers.Conv2D(\n",
    "            1,\n",
    "            4,\n",
    "            strides=1,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(0.0, 0.02),\n",
    "        )\n",
    "\n",
    "    def call(self, x: tf.Tensor, training: bool = False) -> tf.Tensor:\n",
    "        for down in self.down_stack:\n",
    "            x = down(x, training=training)\n",
    "\n",
    "        x = self.zero_pad1(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.batch_norm(x, training=training)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.zero_pad2(x)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"config\": self.config.get_config(), \"name\": self.name}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(\n",
    "            config=ModelConfig.from_config(config[\"config\"]), name=config[\"name\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gan Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, Optional\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"CycleGAN\", name=\"CycleGAN\")\n",
    "class CycleGAN(tf.keras.Model):\n",
    "    def __init__(self, config: ModelConfig, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "\n",
    "        # Generators\n",
    "        self.gen_G = Generator(config, name=\"generator_G\")\n",
    "        self.gen_F = Generator(config, name=\"generator_F\")\n",
    "\n",
    "        # Discriminators\n",
    "        self.disc_X = Discriminator(config, name=\"discriminator_X\")\n",
    "        self.disc_Y = Discriminator(config, name=\"discriminator_Y\")\n",
    "\n",
    "        # Optimizers with same settings\n",
    "        optimizer_kwargs = dict(\n",
    "            learning_rate=config.learning_rate, beta_1=config.beta_1\n",
    "        )\n",
    "        self.gen_G_optimizer = tf.keras.optimizers.Adam(**optimizer_kwargs)\n",
    "        self.gen_F_optimizer = tf.keras.optimizers.Adam(**optimizer_kwargs)\n",
    "        self.disc_X_optimizer = tf.keras.optimizers.Adam(**optimizer_kwargs)\n",
    "        self.disc_Y_optimizer = tf.keras.optimizers.Adam(**optimizer_kwargs)\n",
    "\n",
    "        # Loss trackers\n",
    "        self.gen_G_loss_tracker = tf.keras.metrics.Mean(name=\"gen_G_loss\")\n",
    "        self.gen_F_loss_tracker = tf.keras.metrics.Mean(name=\"gen_F_loss\")\n",
    "        self.disc_X_loss_tracker = tf.keras.metrics.Mean(name=\"disc_X_loss\")\n",
    "        self.disc_Y_loss_tracker = tf.keras.metrics.Mean(name=\"disc_Y_loss\")\n",
    "        self.cycle_loss_tracker = tf.keras.metrics.Mean(name=\"cycle_loss\")\n",
    "        self.identity_loss_tracker = tf.keras.metrics.Mean(name=\"identity_loss\")\n",
    "\n",
    "    def compile(self, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        if isinstance(inputs, tuple):\n",
    "            real_x, real_y = inputs\n",
    "\n",
    "            if training:\n",
    "                fake_y = self.gen_G(real_x, training=training)\n",
    "                fake_x = self.gen_F(real_y, training=training)\n",
    "\n",
    "                return fake_y, fake_x\n",
    "\n",
    "            else:\n",
    "                return self.gen_G(real_x, training=training)\n",
    "\n",
    "        return self.gen_G(inputs, training=training)\n",
    "\n",
    "    def _generator_loss(self, disc_generated_output):\n",
    "        \"\"\"\n",
    "        Calculates the generator loss using the discriminator output.\n",
    "\n",
    "        Args:\n",
    "            disc_generated_output: Discriminator output.\n",
    "\n",
    "        Returns:\n",
    "            The generator loss.\n",
    "        \"\"\"\n",
    "\n",
    "        return tf.reduce_mean(\n",
    "            tf.keras.losses.binary_crossentropy(\n",
    "                tf.ones_like(disc_generated_output),\n",
    "                disc_generated_output,\n",
    "                from_logits=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _discriminator_loss(self, real_output, fake_output):\n",
    "        \"\"\"\n",
    "        Calculates the discriminator loss.\n",
    "\n",
    "        Args:\n",
    "            real_output: Real output.\n",
    "            fake_output: Fake output.\n",
    "\n",
    "        Returns:\n",
    "            The discriminator loss.\n",
    "        \"\"\"\n",
    "\n",
    "        real_loss = tf.keras.losses.binary_crossentropy(\n",
    "            tf.ones_like(real_output), real_output, from_logits=True\n",
    "        )\n",
    "\n",
    "        fake_loss = tf.keras.losses.binary_crossentropy(\n",
    "            tf.zeros_like(fake_output), fake_output, from_logits=True\n",
    "        )\n",
    "\n",
    "        return tf.reduce_mean(real_loss + fake_loss) * 0.5\n",
    "\n",
    "    def _cycle_loss(self, real_image, cycled_image):\n",
    "        \"\"\"\n",
    "        Calculates the consistency loss using the L1 norm.\n",
    "\n",
    "        Args:\n",
    "            real_image: Real image.\n",
    "            cycled_image: Cycled image.\n",
    "\n",
    "        Returns:\n",
    "            The cycle consistency loss.\n",
    "        \"\"\"\n",
    "\n",
    "        return tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "\n",
    "    def _identity_loss(self, real_image, same_image):\n",
    "        \"\"\"\n",
    "        Calculates the identity loss using the L1 norm.\n",
    "\n",
    "        Args:\n",
    "            real_image: Real image.\n",
    "            same_image: Image after identity mapping.\n",
    "\n",
    "        Returns:\n",
    "            The identity loss.\n",
    "        \"\"\"\n",
    "        return tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        if isinstance(batch_data, tuple):\n",
    "            real_x = batch_data[0]\n",
    "            real_y = batch_data[1]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Expected tuple of two tensors, got: {}\".format(batch_data)\n",
    "            )\n",
    "\n",
    "        if real_x.shape != real_y.shape:\n",
    "            raise ValueError(f\"Shape mismatch: {real_x.shape} vs {real_y.shape}\")\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Generator outputs\n",
    "            fake_y = self.gen_G(real_x, training=True)\n",
    "            fake_x = self.gen_F(real_y, training=True)\n",
    "\n",
    "            # Cycle consistency\n",
    "            cycled_x = self.gen_F(fake_y, training=True)\n",
    "            cycled_y = self.gen_G(fake_x, training=True)\n",
    "\n",
    "            # Identity mapping\n",
    "            same_x = self.gen_F(real_x, training=True)\n",
    "            same_y = self.gen_G(real_y, training=True)\n",
    "\n",
    "            # Discriminator outputs\n",
    "            disc_real_x = self.disc_X(real_x, training=True)\n",
    "            disc_fake_x = self.disc_X(fake_x, training=True)\n",
    "            disc_real_y = self.disc_Y(real_y, training=True)\n",
    "            disc_fake_y = self.disc_Y(fake_y, training=True)\n",
    "\n",
    "            # Generator losses\n",
    "            gen_G_loss = self._generator_loss(disc_fake_y)\n",
    "            gen_F_loss = self._generator_loss(disc_fake_x)\n",
    "\n",
    "            # Cycle consistency loss\n",
    "            cycle_loss = (\n",
    "                self._cycle_loss(real_x, cycled_x) + self._cycle_loss(real_y, cycled_y)\n",
    "            ) * self.config.lambda_cycle\n",
    "\n",
    "            # Identity loss\n",
    "            identity_loss = (\n",
    "                self._identity_loss(real_x, same_x)\n",
    "                + self._identity_loss(real_y, same_y)\n",
    "            ) * self.config.lambda_identity\n",
    "\n",
    "            # Total generator losses\n",
    "            total_gen_G_loss = gen_G_loss + cycle_loss + identity_loss\n",
    "            total_gen_F_loss = gen_F_loss + cycle_loss + identity_loss\n",
    "\n",
    "            # Discriminator losses\n",
    "            disc_X_loss = self._discriminator_loss(disc_real_x, disc_fake_x)\n",
    "            disc_Y_loss = self._discriminator_loss(disc_real_y, disc_fake_y)\n",
    "\n",
    "        # Calculate and apply gradients\n",
    "        gen_G_gradients = tape.gradient(\n",
    "            total_gen_G_loss, self.gen_G.trainable_variables\n",
    "        )\n",
    "        gen_F_gradients = tape.gradient(\n",
    "            total_gen_F_loss, self.gen_F.trainable_variables\n",
    "        )\n",
    "        disc_X_gradients = tape.gradient(disc_X_loss, self.disc_X.trainable_variables)\n",
    "        disc_Y_gradients = tape.gradient(disc_Y_loss, self.disc_Y.trainable_variables)\n",
    "\n",
    "        # Apply gradients\n",
    "        self.gen_G_optimizer.apply_gradients(\n",
    "            zip(gen_G_gradients, self.gen_G.trainable_variables)\n",
    "        )\n",
    "        self.gen_F_optimizer.apply_gradients(\n",
    "            zip(gen_F_gradients, self.gen_F.trainable_variables)\n",
    "        )\n",
    "        self.disc_X_optimizer.apply_gradients(\n",
    "            zip(disc_X_gradients, self.disc_X.trainable_variables)\n",
    "        )\n",
    "        self.disc_Y_optimizer.apply_gradients(\n",
    "            zip(disc_Y_gradients, self.disc_Y.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # Update metrics\n",
    "        self.gen_G_loss_tracker.update_state(total_gen_G_loss)\n",
    "        self.gen_F_loss_tracker.update_state(total_gen_F_loss)\n",
    "        self.disc_X_loss_tracker.update_state(disc_X_loss)\n",
    "        self.disc_Y_loss_tracker.update_state(disc_Y_loss)\n",
    "        self.cycle_loss_tracker.update_state(cycle_loss)\n",
    "        self.identity_loss_tracker.update_state(identity_loss)\n",
    "\n",
    "        return {\n",
    "            \"gen_G_loss\": self.gen_G_loss_tracker.result(),\n",
    "            \"gen_F_loss\": self.gen_F_loss_tracker.result(),\n",
    "            \"disc_X_loss\": self.disc_X_loss_tracker.result(),\n",
    "            \"disc_Y_loss\": self.disc_Y_loss_tracker.result(),\n",
    "            \"cycle_loss\": self.cycle_loss_tracker.result(),\n",
    "            \"identity_loss\": self.identity_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def metrics(self) -> list:\n",
    "        \"\"\"\n",
    "        Returns the model's metrics.\n",
    "\n",
    "        Returns:\n",
    "            A list of metrics.\n",
    "        \"\"\"\n",
    "\n",
    "        return [\n",
    "            self.gen_G_loss_tracker,\n",
    "            self.gen_F_loss_tracker,\n",
    "            self.disc_X_loss_tracker,\n",
    "            self.disc_Y_loss_tracker,\n",
    "            self.cycle_loss_tracker,\n",
    "            self.identity_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"config\": self.config.get_config()}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(config=ModelConfig.from_config(config[\"config\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGANEvaluator:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the CycleGAN evaluator with InceptionV3 model.\"\"\"\n",
    "        self.inception_model = tf.keras.applications.InceptionV3(\n",
    "            include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
    "        )\n",
    "\n",
    "    def preprocess_images(self, images):\n",
    "        \"\"\"Preprocess images to the correct format.\"\"\"\n",
    "        # Ensure we're working with float32\n",
    "        images = tf.cast(images, tf.float32)\n",
    "\n",
    "        # Ensure pixel values are in [0, 1]\n",
    "        if tf.reduce_max(images) > 1.0:\n",
    "            images = images / 255.0\n",
    "\n",
    "        return images\n",
    "\n",
    "    def calculate_fid(self, real_images, generated_images):\n",
    "        \"\"\"\n",
    "        Calculate Frechet Inception Distance between real and generated images.\n",
    "\n",
    "        Args:\n",
    "            real_images: Tensor of real images\n",
    "            generated_images: Tensor of generated images\n",
    "\n",
    "        Returns:\n",
    "            FID score (float)\n",
    "        \"\"\"\n",
    "        # Preprocess images\n",
    "        real_images = self.preprocess_images(real_images)\n",
    "        generated_images = self.preprocess_images(generated_images)\n",
    "\n",
    "        # Resize images to InceptionV3 input size\n",
    "        real_images = tf.image.resize(real_images, (299, 299))\n",
    "        generated_images = tf.image.resize(generated_images, (299, 299))\n",
    "\n",
    "        # Ensure batch processing for large datasets\n",
    "        batch_size = 32\n",
    "\n",
    "        def get_features(images):\n",
    "            features_list = []\n",
    "            for i in range(0, len(images), batch_size):\n",
    "                batch = images[i : i + batch_size]\n",
    "                features = self.inception_model.predict(batch, verbose=0)\n",
    "                features_list.append(features)\n",
    "            return np.concatenate(features_list, axis=0)\n",
    "\n",
    "        # Get features\n",
    "        real_features = get_features(real_images)\n",
    "        gen_features = get_features(generated_images)\n",
    "\n",
    "        # Calculate mean and covariance\n",
    "        mu1, sigma1 = np.mean(real_features, axis=0), np.cov(\n",
    "            real_features, rowvar=False\n",
    "        )\n",
    "        mu2, sigma2 = np.mean(gen_features, axis=0), np.cov(gen_features, rowvar=False)\n",
    "\n",
    "        # Calculate FID\n",
    "        ssdiff = np.sum((mu1 - mu2) ** 2.0)\n",
    "        covmean = linalg.sqrtm(sigma1.dot(sigma2), disp=False)[0]\n",
    "\n",
    "        if np.iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "\n",
    "        return float(ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean))\n",
    "\n",
    "    def calculate_ssim(self, real_images, generated_images):\n",
    "        \"\"\"Calculate SSIM between real and generated images.\"\"\"\n",
    "        real_images = self.preprocess_images(real_images)\n",
    "        generated_images = self.preprocess_images(generated_images)\n",
    "\n",
    "        ssim_scores = []\n",
    "        for real, gen in zip(real_images, generated_images):\n",
    "            # Convert to numpy and ensure correct shape\n",
    "            real_np = real.numpy()\n",
    "            gen_np = gen.numpy()\n",
    "\n",
    "            # Handle grayscale images\n",
    "            if len(real_np.shape) == 2:\n",
    "                real_np = real_np[..., np.newaxis]\n",
    "                gen_np = gen_np[..., np.newaxis]\n",
    "\n",
    "            # Use a smaller window size (5x5) and explicitly specify channel_axis\n",
    "            score = structural_similarity(\n",
    "                real_np,\n",
    "                gen_np,\n",
    "                win_size=5,  # Smaller window size\n",
    "                channel_axis=-1,  # Specify channel axis\n",
    "                data_range=1.0,\n",
    "            )\n",
    "            ssim_scores.append(score)\n",
    "\n",
    "        return float(np.mean(ssim_scores))\n",
    "\n",
    "    def calculate_mse(self, real_images, generated_images):\n",
    "        \"\"\"Calculate MSE between real and generated images.\"\"\"\n",
    "        real_images = self.preprocess_images(real_images)\n",
    "        generated_images = self.preprocess_images(generated_images)\n",
    "        return float(tf.reduce_mean(tf.square(real_images - generated_images)))\n",
    "\n",
    "    def calculate_psnr(self, real_images, generated_images):\n",
    "        \"\"\"Calculate PSNR between real and generated images.\"\"\"\n",
    "        mse = self.calculate_mse(real_images, generated_images)\n",
    "        if mse == 0:\n",
    "            return float(\"inf\")\n",
    "        return float(20 * np.log10(1.0 / tf.sqrt(mse)))\n",
    "\n",
    "    def evaluate_model(self, model, test_dataset, num_samples=100):\n",
    "        \"\"\"\n",
    "        Comprehensively evaluate the GAN model using multiple metrics.\n",
    "\n",
    "        Args:\n",
    "            model: CycleGAN model with gen_G attribute\n",
    "            test_dataset: TensorFlow dataset containing test data\n",
    "            num_samples: Number of samples to evaluate\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing evaluation metrics\n",
    "        \"\"\"\n",
    "        # Fix the dataset caching warning by proper ordering of operations\n",
    "        if num_samples is not None:\n",
    "            test_dataset = test_dataset.take(num_samples).cache()\n",
    "        else:\n",
    "            test_dataset = test_dataset.cache()\n",
    "\n",
    "        real_images = []\n",
    "        generated_images = []\n",
    "\n",
    "        try:\n",
    "            # Collect real and generated images\n",
    "            for batch in test_dataset:\n",
    "                if isinstance(batch, tuple):\n",
    "                    real_x = batch[0]\n",
    "                else:\n",
    "                    real_x = batch\n",
    "\n",
    "                # Ensure 4D shape (batch_size, height, width, channels)\n",
    "                if len(real_x.shape) != 4:\n",
    "                    raise ValueError(f\"Expected 4D tensor, got shape: {real_x.shape}\")\n",
    "\n",
    "                generated = model.gen_G(real_x, training=False)\n",
    "\n",
    "                # Ensure consistent shapes\n",
    "                if generated.shape != real_x.shape:\n",
    "                    raise ValueError(\n",
    "                        f\"Shape mismatch: real {real_x.shape} vs generated {generated.shape}\"\n",
    "                    )\n",
    "\n",
    "                real_images.append(real_x)\n",
    "                generated_images.append(generated)\n",
    "\n",
    "            # Convert to tensors\n",
    "            real_images = tf.concat(real_images, axis=0)\n",
    "            generated_images = tf.concat(generated_images, axis=0)\n",
    "\n",
    "            print(f\"Evaluating {len(real_images)} image pairs...\")\n",
    "\n",
    "            # Calculate all metrics\n",
    "            metrics = {\n",
    "                \"fid\": self.calculate_fid(real_images, generated_images),\n",
    "                \"ssim\": self.calculate_ssim(real_images, generated_images),\n",
    "                \"mse\": self.calculate_mse(real_images, generated_images),\n",
    "                \"psnr\": self.calculate_psnr(real_images, generated_images),\n",
    "            }\n",
    "\n",
    "            return metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error during evaluation:\")\n",
    "            print(f\"Original error: {str(e)}\")\n",
    "            if len(real_images) > 0:\n",
    "                print(\"\\nTensor shapes in the batch where error occurred:\")\n",
    "                print(f\"Real images shape: {real_images[-1].shape}\")\n",
    "                print(f\"Generated images shape: {generated_images[-1].shape}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsVisualizer:\n",
    "    def __init__(self, save_dir=\"./evaluation_results\"):\n",
    "        \"\"\"\n",
    "        Initialize the metrics visualizer.\n",
    "\n",
    "        Args:\n",
    "            save_dir: Directory to save the plots\n",
    "        \"\"\"\n",
    "        self.save_dir = save_dir\n",
    "        tf.io.gfile.makedirs(save_dir)\n",
    "\n",
    "    def plot_image_comparison(self, real_images, generated_images, num_samples=5):\n",
    "        \"\"\"Plot a grid of real vs generated images.\"\"\"\n",
    "        plt.figure(figsize=(15, 6))\n",
    "\n",
    "        # Randomly sample image pairs\n",
    "        total_images = len(real_images)\n",
    "        indices = np.random.choice(total_images, num_samples, replace=False)\n",
    "\n",
    "        for idx, i in enumerate(indices):\n",
    "            # Plot real image\n",
    "            plt.subplot(2, num_samples, idx + 1)\n",
    "            plt.imshow(self._prepare_image_for_plot(real_images[i]))\n",
    "            plt.axis(\"off\")\n",
    "            if idx == 0:\n",
    "                plt.title(\"Real Images\")\n",
    "\n",
    "            # Plot generated image\n",
    "            plt.subplot(2, num_samples, num_samples + idx + 1)\n",
    "            plt.imshow(self._prepare_image_for_plot(generated_images[i]))\n",
    "            plt.axis(\"off\")\n",
    "            if idx == 0:\n",
    "                plt.title(\"Generated Images\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        plt.savefig(f\"{self.save_dir}/image_comparison_{timestamp}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def plot_metrics_history(self, metrics_history):\n",
    "        \"\"\"Plot the evolution of metrics over time.\"\"\"\n",
    "        plt.figure(figsize=(15, 10))\n",
    "\n",
    "        # Create subplots for each metric\n",
    "        metrics = list(metrics_history[0].keys())\n",
    "        num_metrics = len(metrics)\n",
    "        rows = (num_metrics + 1) // 2\n",
    "\n",
    "        for idx, metric in enumerate(metrics, 1):\n",
    "            plt.subplot(rows, 2, idx)\n",
    "            values = [h[metric] for h in metrics_history]\n",
    "            epochs = range(1, len(values) + 1)\n",
    "\n",
    "            plt.plot(epochs, values, \"b-\", marker=\"o\")\n",
    "            plt.title(f\"{metric.upper()} over Time\")\n",
    "            plt.xlabel(\"Evaluation Step\")\n",
    "            plt.ylabel(metric.upper())\n",
    "            plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        plt.savefig(f\"{self.save_dir}/metrics_history_{timestamp}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def plot_metrics_distribution(self, real_images, generated_images, evaluator):\n",
    "        \"\"\"Plot the distribution of metrics across individual image pairs.\"\"\"\n",
    "        plt.figure(figsize=(15, 10))\n",
    "\n",
    "        # Calculate metrics for each image pair\n",
    "        ssim_scores = []\n",
    "        mse_scores = []\n",
    "        psnr_scores = []\n",
    "\n",
    "        for real, gen in zip(real_images, generated_images):\n",
    "            real_batch = tf.expand_dims(real, 0)\n",
    "            gen_batch = tf.expand_dims(gen, 0)\n",
    "\n",
    "            ssim = evaluator.calculate_ssim(real_batch, gen_batch)\n",
    "            mse = evaluator.calculate_mse(real_batch, gen_batch)\n",
    "            psnr = evaluator.calculate_psnr(real_batch, gen_batch)\n",
    "\n",
    "            ssim_scores.append(ssim)\n",
    "            mse_scores.append(mse)\n",
    "            psnr_scores.append(psnr)\n",
    "\n",
    "        # Plot distributions\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.hist(ssim_scores, bins=30, color=\"blue\", alpha=0.7)\n",
    "        plt.title(\"SSIM Distribution\")\n",
    "        plt.xlabel(\"SSIM Score\")\n",
    "        plt.ylabel(\"Count\")\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.hist(mse_scores, bins=30, color=\"red\", alpha=0.7)\n",
    "        plt.title(\"MSE Distribution\")\n",
    "        plt.xlabel(\"MSE Score\")\n",
    "        plt.ylabel(\"Count\")\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.hist(psnr_scores, bins=30, color=\"green\", alpha=0.7)\n",
    "        plt.title(\"PSNR Distribution\")\n",
    "        plt.xlabel(\"PSNR Score\")\n",
    "        plt.ylabel(\"Count\")\n",
    "\n",
    "        # Add summary statistics\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.axis(\"off\")\n",
    "        summary_text = (\n",
    "            f\"Summary Statistics:\\n\\n\"\n",
    "            f\"SSIM:\\n\"\n",
    "            f\"  Mean: {np.mean(ssim_scores):.4f}\\n\"\n",
    "            f\"  Std: {np.std(ssim_scores):.4f}\\n\\n\"\n",
    "            f\"MSE:\\n\"\n",
    "            f\"  Mean: {np.mean(mse_scores):.4f}\\n\"\n",
    "            f\"  Std: {np.std(mse_scores):.4f}\\n\\n\"\n",
    "            f\"PSNR:\\n\"\n",
    "            f\"  Mean: {np.mean(psnr_scores):.4f}\\n\"\n",
    "            f\"  Std: {np.std(psnr_scores):.4f}\"\n",
    "        )\n",
    "        plt.text(0.1, 0.1, summary_text, fontsize=10, fontfamily=\"monospace\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        plt.savefig(f\"{self.save_dir}/metrics_distribution_{timestamp}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def _prepare_image_for_plot(self, image):\n",
    "        \"\"\"Prepare image for plotting.\"\"\"\n",
    "        # Convert to numpy array if tensor\n",
    "        if isinstance(image, tf.Tensor):\n",
    "            image = image.numpy()\n",
    "\n",
    "        # Ensure values are in [0, 1]\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "\n",
    "        # Clip values to [0, 1]\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation_results(\n",
    "    evaluator, model, test_dataset, num_samples=100, metrics_history=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Comprehensive plotting of evaluation results.\n",
    "\n",
    "    Args:\n",
    "        evaluator: CycleGANEvaluator instance\n",
    "        model: CycleGAN model\n",
    "        test_dataset: Test dataset\n",
    "        num_samples: Number of samples to evaluate\n",
    "        metrics_history: List of metrics dictionaries over time (optional)\n",
    "    \"\"\"\n",
    "    visualizer = MetricsVisualizer()\n",
    "\n",
    "    # Collect images and calculate metrics\n",
    "    real_images = []\n",
    "    generated_images = []\n",
    "\n",
    "    # Take and cache the dataset\n",
    "    dataset = test_dataset.take(num_samples).cache()\n",
    "\n",
    "    for batch in dataset:\n",
    "        if isinstance(batch, tuple):\n",
    "            real_x = batch[0]\n",
    "        else:\n",
    "            real_x = batch\n",
    "\n",
    "        generated = model.gen_G(real_x, training=False)\n",
    "        real_images.extend(tf.unstack(real_x))\n",
    "        generated_images.extend(tf.unstack(generated))\n",
    "\n",
    "    # Convert to tensors\n",
    "    real_images = tf.stack(real_images)\n",
    "    generated_images = tf.stack(generated_images)\n",
    "\n",
    "    # Plot image comparisons\n",
    "    visualizer.plot_image_comparison(real_images, generated_images)\n",
    "\n",
    "    # Plot metrics distribution\n",
    "    visualizer.plot_metrics_distribution(real_images, generated_images, evaluator)\n",
    "\n",
    "    # Plot metrics history if provided\n",
    "    if metrics_history is not None:\n",
    "        visualizer.plot_metrics_history(metrics_history)\n",
    "\n",
    "    print(f\"Plots have been saved to {visualizer.save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGANVisualizer:\n",
    "    \"\"\"Visualization tools for CycleGAN results.\"\"\"\n",
    "\n",
    "    def __init__(self, model: \"CycleGAN\"):\n",
    "        self.model = model\n",
    "        self.utils = VisualizationUtils()\n",
    "\n",
    "    def display_samples(\n",
    "        self,\n",
    "        dataset: tf.data.Dataset,\n",
    "        num_samples: Tuple[int, int] = (4, 6),\n",
    "        figsize: Optional[Tuple[int, int]] = None,\n",
    "        save_path: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Display or save a grid of samples from the dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset: TensorFlow dataset containing the images\n",
    "            num_samples: Tuple of (rows, columns) for the display grid\n",
    "            figsize: Optional tuple for figure size\n",
    "            save_path: Optional path to save the figure\n",
    "        \"\"\"\n",
    "        rows, cols = num_samples\n",
    "        fig, axes = self.utils.create_figure(rows, cols, figsize)\n",
    "\n",
    "        samples = next(iter(dataset.take(1).batch(rows * cols)))\n",
    "\n",
    "        for idx, (ax, img) in enumerate(zip(axes, samples)):\n",
    "            if isinstance(img, tuple):\n",
    "                img = img[0]\n",
    "\n",
    "            # Display image\n",
    "            ax.imshow(self.utils.denormalize_image(img))\n",
    "            ax.axis(\"off\")\n",
    "            ax.set_title(f\"Sample {idx + 1}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "    def display_generated_samples(\n",
    "        self,\n",
    "        test_dataset: tf.data.Dataset,\n",
    "        num_samples: int = 3,\n",
    "        figsize: Optional[Tuple[int, int]] = None,\n",
    "        save_path: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Display or save pairs of input and generated images.\n",
    "\n",
    "        Args:\n",
    "            test_dataset: Dataset containing test images\n",
    "            num_samples: Number of sample pairs to display\n",
    "            figsize: Optional tuple for figure size\n",
    "            save_path: Optional path to save the figure\n",
    "        \"\"\"\n",
    "        if figsize is None:\n",
    "            figsize = (8 * 2, 4 * num_samples)\n",
    "\n",
    "        fig, axes = plt.subplots(num_samples, 2, figsize=figsize)\n",
    "        if num_samples == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "\n",
    "        for idx, samples in enumerate(test_dataset.take(num_samples)):\n",
    "            if isinstance(samples, tuple):\n",
    "                input_image = samples[0]\n",
    "            else:\n",
    "                input_image = samples\n",
    "\n",
    "            # Generate image\n",
    "            generated_image = self.model.gen_G(input_image, training=False)\n",
    "\n",
    "            axes[idx, 0].imshow(self.utils.denormalize_image(input_image[0]))\n",
    "            axes[idx, 0].axis(\"off\")\n",
    "            axes[idx, 0].set_title(\"Input Photo\")\n",
    "\n",
    "            axes[idx, 1].imshow(self.utils.denormalize_image(generated_image[0]))\n",
    "            axes[idx, 1].axis(\"off\")\n",
    "            axes[idx, 1].set_title(\"Generated Monet\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "    def display_progress(\n",
    "        self,\n",
    "        test_image: Union[str, tf.Tensor],\n",
    "        epoch: int,\n",
    "        save_dir: str = \"training_progress\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Generate and save a Monet-style image to track training progress.\n",
    "\n",
    "        Args:\n",
    "            test_image: Path to image or tensor of test image\n",
    "            epoch: Current epoch number\n",
    "            save_dir: Directory to save progress images\n",
    "        \"\"\"\n",
    "        import os\n",
    "\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        if isinstance(test_image, str):\n",
    "            img = tf.keras.preprocessing.image.load_img(\n",
    "                test_image,\n",
    "                target_size=(self.model.config.height, self.model.config.width),\n",
    "            )\n",
    "\n",
    "            img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "            img = (img / 127.5) - 1.0\n",
    "            img = tf.expand_dims(img, 0)\n",
    "\n",
    "        else:\n",
    "            img = test_image\n",
    "\n",
    "        generated = self.model.gen_G(img, training=False)\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "        ax1.imshow(self.utils.denormalize_image(img[0]))\n",
    "        ax1.axis(\"off\")\n",
    "        ax1.set_title(\"Original Photo\")\n",
    "\n",
    "        ax2.imshow(self.utils.denormalize_image(generated[0]))\n",
    "        ax2.axis(\"off\")\n",
    "        ax2.set_title(f\"Generated Monet (Epoch {epoch})\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            os.path.join(save_dir, f\"progress_epoch_{epoch}.png\"),\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=300,\n",
    "        )\n",
    "\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training(base_dir: str = \".\", batch_size: int = 1):\n",
    "    \"\"\"\n",
    "    Setups the training data and configuration.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): The base directory containing the data.\n",
    "        batch_size (int): The batch size.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the configuration, training dataset, test dataset, and steps per epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    config = ModelConfig(\n",
    "        height=256,\n",
    "        width=256,\n",
    "        channels=3,\n",
    "        base_filters=64,\n",
    "        lambda_cycle=10.0,\n",
    "        lambda_identity=0.5,\n",
    "        learning_rate=2e-4,\n",
    "        beta_1=0.5,\n",
    "    )\n",
    "\n",
    "    data_dir = Path(base_dir) / \"data\"\n",
    "    monet_files = tf.io.gfile.glob(str(data_dir / \"monet_tfrec\" / \"*.tfrec\"))\n",
    "    photo_files = tf.io.gfile.glob(str(data_dir / \"photo_tfrec\" / \"*.tfrec\"))\n",
    "\n",
    "    if not monet_files or not photo_files:\n",
    "        raise ValueError(\"No TFRecord files found in \", data_dir)\n",
    "\n",
    "    n_monet = sum(1 for f in monet_files for _ in tf.data.TFRecordDataset(f))\n",
    "    n_photo = sum(1 for f in photo_files for _ in tf.data.TFRecordDataset(f))\n",
    "    print(f\"Found {n_monet} Monet images and {n_photo} photos\")\n",
    "\n",
    "    processor = ImageProcessor(config)\n",
    "    monet_ds = processor.create_dataset(\n",
    "        monet_files, batch_size=batch_size, shuffle=True, cache=True\n",
    "    )\n",
    "\n",
    "    photo_ds = processor.create_dataset(\n",
    "        photo_files, batch_size=batch_size, shuffle=True, cache=True\n",
    "    )\n",
    "\n",
    "    test_ds = processor.create_dataset(\n",
    "        photo_files[:10],\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        cache=True,\n",
    "    )\n",
    "\n",
    "    train_ds = tf.data.Dataset.zip((photo_ds, monet_ds))\n",
    "    steps_per_epoch = min(n_monet, n_photo) // batch_size\n",
    "\n",
    "    return config, train_ds, test_ds, steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_training(config: ModelConfig, strategy):\n",
    "    \"\"\"\n",
    "    Setups the CycleGAN model and training configuration.\n",
    "\n",
    "    Args:\n",
    "        config (ModelConfig): The model configuration.\n",
    "        strategy: The distribution strategy.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the model and training callbacks.\n",
    "    \"\"\"\n",
    "\n",
    "    Path(\"logs/cyclegan\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"checkpoints/cyclegan\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with strategy.scope():\n",
    "        model = CycleGAN(config)\n",
    "        model.compile()\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=f\"logs/cyclegan/{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=\"checkpoints/cyclegan/model.{epoch:03d}.weights.h5\",\n",
    "            save_freq=\"epoch\",\n",
    "            save_weights_only=True,\n",
    "            monitor=\"gen_G_loss\",\n",
    "            mode=\"min\",\n",
    "            save_best_only=True,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    return model, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 09:55:20.567020: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:370] TFRecordDataset `buffer_size` is unspecified, default to 262144\n",
      "2024-11-14 09:55:20.576787: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-14 09:55:20.595942: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-14 09:55:20.632646: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-14 09:55:20.754711: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-14 09:55:21.024572: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300 Monet images and 7038 photos\n"
     ]
    }
   ],
   "source": [
    "config, train_ds, test_ds, steps_per_epoch = setup_training(\n",
    "    base_dir=\"../\", batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, callbacks = setup_model_and_training(config, strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos/miniconda3/envs/ml/lib/python3.10/site-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'cycle_gan', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1731599751.726755    3300 meta_optimizer.cc:966] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/generator_G_5/upsample_block_1/dropout_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "I0000 00:00:1731599755.857018   13866 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m295/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 361ms/step - cycle_loss: 7.0293 - disc_X_loss: 0.6110 - disc_Y_loss: 0.6235 - gen_F_loss: 8.1650 - gen_G_loss: 8.1606 - identity_loss: 0.3535"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 09:57:49.789822: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m299/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361ms/step - cycle_loss: 7.0133 - disc_X_loss: 0.6107 - disc_Y_loss: 0.6237 - gen_F_loss: 8.1489 - gen_G_loss: 8.1436 - identity_loss: 0.3526"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos/miniconda3/envs/ml/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 367ms/step - cycle_loss: 7.0093 - disc_X_loss: 0.6106 - disc_Y_loss: 0.6238 - gen_F_loss: 8.1448 - gen_G_loss: 8.1394 - identity_loss: 0.3524\n",
      "Epoch 2/2\n",
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 408ms/step - cycle_loss: 5.2181 - disc_X_loss: 0.4439 - disc_Y_loss: 0.6240 - gen_F_loss: 6.6434 - gen_G_loss: 6.2898 - identity_loss: 0.2529"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 09:59:54.620337: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 414ms/step - cycle_loss: 5.2159 - disc_X_loss: 0.4439 - disc_Y_loss: 0.6237 - gen_F_loss: 6.6423 - gen_G_loss: 6.2883 - identity_loss: 0.2528\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=2,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = CycleGANVisualizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 09:59:58.765759: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "visualizer.display_generated_samples(\n",
    "    test_ds, num_samples=8, save_path=\"generated_samples.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = CycleGANEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 10:00:07.033483: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-11-14 10:00:07.067546: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 100 image pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731600008.442807   13865 service.cc:148] XLA service 0x7facdc219d30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1731600008.443264   13865 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2024-11-14 10:00:08.569956: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-14 10:00:11.430597: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 593.83MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "I0000 00:00:1731600019.960543   13865 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-11-14 10:00:42.081051: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots have been saved to ./evaluation_results\n",
      "Fid: 117.6134\n",
      "Ssim: 0.3047\n",
      "Mse: 0.0879\n",
      "Psnr: 10.5605\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    metrics = evaluator.evaluate_model(model, test_ds, num_samples=100)\n",
    "    metrics_history.append(metrics)\n",
    "\n",
    "    plot_evaluation_results(\n",
    "        evaluator=evaluator,\n",
    "        model=model,\n",
    "        test_dataset=test_ds,\n",
    "        num_samples=100,\n",
    "        metrics_history=metrics_history,\n",
    "    )\n",
    "\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name.capitalize()}: {value:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"cyclegan_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"cyclegan_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "generator_g = model.gen_G\n",
    "generator_g.save(\"monet_generator.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: generator_saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: generator_saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "@tf.function(\n",
    "    input_signature=[tf.TensorSpec(shape=[None, 256, 256, 3], dtype=tf.float32)]\n",
    ")\n",
    "def serve(input_img):\n",
    "    return generator_g(input_img)\n",
    "\n",
    "\n",
    "# Save with signatures\n",
    "tf.saved_model.save(\n",
    "    generator_g, \"generator_saved_model\", signatures={\"serving_default\": serve}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "generator_g.save(\"monet_generator.keras\")\n",
    "generator_g.save(\"monet_generator.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: monet_generator/saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: monet_generator/saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(generator_g, \"monet_generator/saved_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
