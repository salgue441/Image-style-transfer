{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:24:37.858623: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-12 15:24:37.868466: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731446677.880903   16205 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731446677.884331   16205 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-12 15:24:37.896388: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CycleGAN\n",
    "\n",
    "### Overview\n",
    "\n",
    "This implementation provides a complete TensorFlow-based CycleGAN framework for image-to-image translation. The codebase includes data processing, model architecture, training pipeline, and evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731446679.456299   16205 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpu_devices:\n",
    "    for device in gpu_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n",
    "\n",
    "else:\n",
    "    print(\"No GPU devices found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    height: int = 256\n",
    "    width: int = 256\n",
    "    channels: int = 3\n",
    "    base_filters: int = 64\n",
    "    lambda_cycle: float = 10.0\n",
    "    lambda_identity: float = 0.5\n",
    "    learning_rate: float = 2e-4\n",
    "    beta_1: float = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProcessor:\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def decode_image(self, image: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Decodes and normalizes the image.\n",
    "\n",
    "        Args:\n",
    "          image: A tensor representing an image.\n",
    "\n",
    "        Returns:\n",
    "          The decoded and normalized image.\n",
    "        \"\"\"\n",
    "\n",
    "        image = tf.image.decode_jpeg(image, channels=self.config.channels)\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image = (image / 127.5) - 1\n",
    "\n",
    "        return image\n",
    "\n",
    "    @tf.function\n",
    "    def parse_tfrecord(self, example_photo):\n",
    "        \"\"\"\n",
    "        Parses a TFRecord example containing a photo.\n",
    "\n",
    "        Args:\n",
    "          example_photo: A TFRecord example containing a photo.\n",
    "\n",
    "        Returns:\n",
    "          The decoded and normalized photo.\n",
    "        \"\"\"\n",
    "\n",
    "        feature_description = {\n",
    "            \"image_name\": tf.io.FixedLenFeature([], tf.string),\n",
    "            \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "            \"target\": tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "\n",
    "        example = tf.io.parse_single_example(example_photo, feature_description)\n",
    "        return self.decode_image(example[\"image\"])\n",
    "\n",
    "    def create_dataset(\n",
    "        self,\n",
    "        filenames: list,\n",
    "        batch_size: int = 1,\n",
    "        shuffle: bool = True,\n",
    "        cache: bool = True,\n",
    "    ) -> tf.data.Dataset:\n",
    "        \"\"\"\n",
    "        Creates a dataset from TFRecord files.\n",
    "\n",
    "        Args:\n",
    "            filenames (list): A list of TFRecord filenames.\n",
    "            batch_size (int): The batch size.\n",
    "            shuffle (bool): Whether to shuffle the dataset.\n",
    "            cache (bool): Whether to cache the dataset in memory.\n",
    "\n",
    "        Returns:\n",
    "            A tf.data.Dataset instance.\n",
    "        \"\"\"\n",
    "\n",
    "        dataset = tf.data.TFRecordDataset(\n",
    "            filenames, num_parallel_reads=tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "\n",
    "        dataset = dataset.map(\n",
    "            self.parse_tfrecord, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=2048, reshuffle_each_iteration=True)\n",
    "\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "        if cache:\n",
    "            dataset = dataset.cache()\n",
    "\n",
    "        return dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualizationUtils:\n",
    "    @staticmethod\n",
    "    def denormalize_image(image: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Denormalizes the image by scaling it back to the [0, 255] range.\n",
    "\n",
    "        Args:\n",
    "            image: A tensor representing an image.\n",
    "\n",
    "        Returns:\n",
    "            The denormalized image.\n",
    "        \"\"\"\n",
    "\n",
    "        return (image * 0.5) + 0.5\n",
    "\n",
    "    @staticmethod\n",
    "    def create_figure(\n",
    "        nrows: int, ncols: int, figsize: Optional[Tuple[int, int]] = None\n",
    "    ) -> Tuple[plt.Figure, plt.Axes]:\n",
    "        \"\"\"\n",
    "        Create a Matplotlib figure and axes with the given number of rows and columns.\n",
    "\n",
    "        Args:\n",
    "            nrows (int): The number of rows.\n",
    "            ncols (int): The number of columns.\n",
    "            figsize (tuple): The figure size.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the figure and axes.\n",
    "        \"\"\"\n",
    "\n",
    "        if figsize is None:\n",
    "            figsize = (ncols * 4, nrows * 4)\n",
    "\n",
    "        fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "\n",
    "        if nrows * ncols == 1:\n",
    "            axes = np.array([axes])\n",
    "\n",
    "        return fig, axes.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class DownsampleBlock(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filters: int,\n",
    "        size: int = 4,\n",
    "        strides: int = 2,\n",
    "        apply_norm: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.conv = tf.keras.layers.Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=size,\n",
    "            strides=strides,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "                mean=0.0, stddev=0.02\n",
    "            ),\n",
    "            use_bias=not apply_norm,\n",
    "        )\n",
    "\n",
    "        self.batch_norm = tf.keras.layers.BatchNormalization(\n",
    "            gamma_initializer=(\n",
    "                tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "                if apply_norm\n",
    "                else None\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.activation = tf.keras.layers.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "    def call(self, x: tf.Tensor, training: bool = True) -> tf.Tensor:\n",
    "        x = self.conv(x)\n",
    "        if self.batch_norm is not None:\n",
    "            x = self.batch_norm(x, training=training)\n",
    "\n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class UpsampleBlock(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filters: int,\n",
    "        size: int = 4,\n",
    "        strides: int = 2,\n",
    "        apply_dropout: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.conv_transpose = tf.keras.layers.Conv2DTranspose(\n",
    "            filters=filters,\n",
    "            kernel_size=size,\n",
    "            strides=strides,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "                mean=0.0, stddev=0.02\n",
    "            ),\n",
    "            use_bias=False,\n",
    "        )\n",
    "\n",
    "        self.batch_norm = tf.keras.layers.BatchNormalization(\n",
    "            gamma_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "        )\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5) if apply_dropout else None\n",
    "        self.activation = tf.keras.layers.ReLU()\n",
    "\n",
    "    def call(self, x: tf.Tensor, training: bool = True) -> tf.Tensor:\n",
    "        x = self.conv_transpose(x)\n",
    "        x = self.batch_norm(x, training=training)\n",
    "\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x, training=training)\n",
    "\n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, config: ModelConfig, name: str = \"generator\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.config = config\n",
    "\n",
    "        self.downsample_stack = [\n",
    "            DownsampleBlock(64, 4, apply_norm=False),\n",
    "            DownsampleBlock(128, 4),\n",
    "            DownsampleBlock(256, 4),\n",
    "            DownsampleBlock(512, 4),\n",
    "            DownsampleBlock(512, 4),\n",
    "            DownsampleBlock(512, 4),\n",
    "            DownsampleBlock(512, 4),\n",
    "            DownsampleBlock(512, 4),\n",
    "        ]\n",
    "\n",
    "        self.upsample_stack = [\n",
    "            UpsampleBlock(512, 4, apply_dropout=True),\n",
    "            UpsampleBlock(512, 4, apply_dropout=True),\n",
    "            UpsampleBlock(512, 4, apply_dropout=True),\n",
    "            UpsampleBlock(512, 4),\n",
    "            UpsampleBlock(256, 4),\n",
    "            UpsampleBlock(128, 4),\n",
    "            UpsampleBlock(64, 4),\n",
    "        ]\n",
    "\n",
    "        self.final_conv = tf.keras.layers.Conv2DTranspose(\n",
    "            filters=config.channels,\n",
    "            kernel_size=4,\n",
    "            strides=2,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "                mean=0.0, stddev=0.02\n",
    "            ),\n",
    "            activation=\"tanh\",\n",
    "        )\n",
    "\n",
    "    def call(self, x: tf.Tensor, training: bool = False) -> tf.Tensor:\n",
    "        skips = []\n",
    "        for down in self.downsample_stack:\n",
    "            x = down(x, training=training)\n",
    "            skips.append(x)\n",
    "\n",
    "        skips = reversed(skips[:-1])\n",
    "\n",
    "        for up, skip in zip(self.upsample_stack, skips):\n",
    "            x = up(x, training=training)\n",
    "            x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class Discriminator(tf.keras.Model):\n",
    "    \"\"\"Modern implementation of PatchGAN discriminator.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ModelConfig, name: str = \"discriminator\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.config = config\n",
    "\n",
    "        self.down_stack = [\n",
    "            DownsampleBlock(config.base_filters, apply_norm=False),\n",
    "            DownsampleBlock(config.base_filters * 2),\n",
    "            DownsampleBlock(config.base_filters * 4),\n",
    "        ]\n",
    "\n",
    "        self.zero_pad1 = tf.keras.layers.ZeroPadding2D()\n",
    "        self.conv = tf.keras.layers.Conv2D(\n",
    "            config.base_filters * 8,\n",
    "            4,\n",
    "            strides=1,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(0.0, 0.02),\n",
    "            use_bias=False,\n",
    "        )\n",
    "        self.batch_norm = tf.keras.layers.BatchNormalization(\n",
    "            gamma_initializer=tf.keras.initializers.RandomNormal(0.0, 0.02)\n",
    "        )\n",
    "        self.leaky_relu = tf.keras.layers.LeakyReLU(0.2)\n",
    "        self.zero_pad2 = tf.keras.layers.ZeroPadding2D()\n",
    "        self.final_conv = tf.keras.layers.Conv2D(\n",
    "            1,\n",
    "            4,\n",
    "            strides=1,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(0.0, 0.02),\n",
    "        )\n",
    "\n",
    "    def call(self, x: tf.Tensor, training: bool = False) -> tf.Tensor:\n",
    "        for down in self.down_stack:\n",
    "            x = down(x, training=training)\n",
    "\n",
    "        x = self.zero_pad1(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.batch_norm(x, training=training)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.zero_pad2(x)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gan Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, Optional\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class CycleGAN(tf.keras.Model):\n",
    "    def __init__(self, config: ModelConfig, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "\n",
    "        # Generators\n",
    "        self.gen_G = Generator(config, name=\"generator_G\")\n",
    "        self.gen_F = Generator(config, name=\"generator_F\")\n",
    "\n",
    "        # Discriminators\n",
    "        self.disc_X = Discriminator(config, name=\"discriminator_X\")\n",
    "        self.disc_Y = Discriminator(config, name=\"discriminator_Y\")\n",
    "\n",
    "        # Optimizers with same settings\n",
    "        optimizer_kwargs = dict(\n",
    "            learning_rate=config.learning_rate, beta_1=config.beta_1\n",
    "        )\n",
    "        self.gen_G_optimizer = tf.keras.optimizers.Adam(**optimizer_kwargs)\n",
    "        self.gen_F_optimizer = tf.keras.optimizers.Adam(**optimizer_kwargs)\n",
    "        self.disc_X_optimizer = tf.keras.optimizers.Adam(**optimizer_kwargs)\n",
    "        self.disc_Y_optimizer = tf.keras.optimizers.Adam(**optimizer_kwargs)\n",
    "\n",
    "        # Loss trackers\n",
    "        self.gen_G_loss_tracker = tf.keras.metrics.Mean(name=\"gen_G_loss\")\n",
    "        self.gen_F_loss_tracker = tf.keras.metrics.Mean(name=\"gen_F_loss\")\n",
    "        self.disc_X_loss_tracker = tf.keras.metrics.Mean(name=\"disc_X_loss\")\n",
    "        self.disc_Y_loss_tracker = tf.keras.metrics.Mean(name=\"disc_Y_loss\")\n",
    "        self.cycle_loss_tracker = tf.keras.metrics.Mean(name=\"cycle_loss\")\n",
    "        self.identity_loss_tracker = tf.keras.metrics.Mean(name=\"identity_loss\")\n",
    "\n",
    "    def compile(self, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        if isinstance(inputs, tuple):\n",
    "            real_x, real_y = inputs\n",
    "\n",
    "            if training:\n",
    "                fake_y = self.gen_G(real_x, training=training)\n",
    "                fake_x = self.gen_F(real_y, training=training)\n",
    "\n",
    "                return fake_y, fake_x\n",
    "\n",
    "            else:\n",
    "                return self.gen_G(real_x, training=training)\n",
    "\n",
    "        return self.gen_G(inputs, training=training)\n",
    "\n",
    "    def _generator_loss(self, disc_generated_output):\n",
    "        \"\"\"\n",
    "        Calculates the generator loss using the discriminator output.\n",
    "\n",
    "        Args:\n",
    "            disc_generated_output: Discriminator output.\n",
    "\n",
    "        Returns:\n",
    "            The generator loss.\n",
    "        \"\"\"\n",
    "\n",
    "        return tf.reduce_mean(\n",
    "            tf.keras.losses.binary_crossentropy(\n",
    "                tf.ones_like(disc_generated_output),\n",
    "                disc_generated_output,\n",
    "                from_logits=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _discriminator_loss(self, real_output, fake_output):\n",
    "        \"\"\"\n",
    "        Calculates the discriminator loss.\n",
    "\n",
    "        Args:\n",
    "            real_output: Real output.\n",
    "            fake_output: Fake output.\n",
    "\n",
    "        Returns:\n",
    "            The discriminator loss.\n",
    "        \"\"\"\n",
    "\n",
    "        real_loss = tf.keras.losses.binary_crossentropy(\n",
    "            tf.ones_like(real_output), real_output, from_logits=True\n",
    "        )\n",
    "\n",
    "        fake_loss = tf.keras.losses.binary_crossentropy(\n",
    "            tf.zeros_like(fake_output), fake_output, from_logits=True\n",
    "        )\n",
    "\n",
    "        return tf.reduce_mean(real_loss + fake_loss) * 0.5\n",
    "\n",
    "    def _cycle_loss(self, real_image, cycled_image):\n",
    "        \"\"\"\n",
    "        Calculates the consistency loss using the L1 norm.\n",
    "\n",
    "        Args:\n",
    "            real_image: Real image.\n",
    "            cycled_image: Cycled image.\n",
    "\n",
    "        Returns:\n",
    "            The cycle consistency loss.\n",
    "        \"\"\"\n",
    "\n",
    "        return tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "\n",
    "    def _identity_loss(self, real_image, same_image):\n",
    "        \"\"\"\n",
    "        Calculates the identity loss using the L1 norm.\n",
    "\n",
    "        Args:\n",
    "            real_image: Real image.\n",
    "            same_image: Image after identity mapping.\n",
    "\n",
    "        Returns:\n",
    "            The identity loss.\n",
    "        \"\"\"\n",
    "        return tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        if isinstance(batch_data, tuple):\n",
    "            real_x = batch_data[0]\n",
    "            real_y = batch_data[1]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Expected tuple of two tensors, got: {}\".format(batch_data)\n",
    "            )\n",
    "\n",
    "        if real_x.shape != real_y.shape:\n",
    "            raise ValueError(f\"Shape mismatch: {real_x.shape} vs {real_y.shape}\")\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Generator outputs\n",
    "            fake_y = self.gen_G(real_x, training=True)\n",
    "            fake_x = self.gen_F(real_y, training=True)\n",
    "\n",
    "            # Cycle consistency\n",
    "            cycled_x = self.gen_F(fake_y, training=True)\n",
    "            cycled_y = self.gen_G(fake_x, training=True)\n",
    "\n",
    "            # Identity mapping\n",
    "            same_x = self.gen_F(real_x, training=True)\n",
    "            same_y = self.gen_G(real_y, training=True)\n",
    "\n",
    "            # Discriminator outputs\n",
    "            disc_real_x = self.disc_X(real_x, training=True)\n",
    "            disc_fake_x = self.disc_X(fake_x, training=True)\n",
    "            disc_real_y = self.disc_Y(real_y, training=True)\n",
    "            disc_fake_y = self.disc_Y(fake_y, training=True)\n",
    "\n",
    "            # Generator losses\n",
    "            gen_G_loss = self._generator_loss(disc_fake_y)\n",
    "            gen_F_loss = self._generator_loss(disc_fake_x)\n",
    "\n",
    "            # Cycle consistency loss\n",
    "            cycle_loss = (\n",
    "                self._cycle_loss(real_x, cycled_x) + self._cycle_loss(real_y, cycled_y)\n",
    "            ) * self.config.lambda_cycle\n",
    "\n",
    "            # Identity loss\n",
    "            identity_loss = (\n",
    "                self._identity_loss(real_x, same_x)\n",
    "                + self._identity_loss(real_y, same_y)\n",
    "            ) * self.config.lambda_identity\n",
    "\n",
    "            # Total generator losses\n",
    "            total_gen_G_loss = gen_G_loss + cycle_loss + identity_loss\n",
    "            total_gen_F_loss = gen_F_loss + cycle_loss + identity_loss\n",
    "\n",
    "            # Discriminator losses\n",
    "            disc_X_loss = self._discriminator_loss(disc_real_x, disc_fake_x)\n",
    "            disc_Y_loss = self._discriminator_loss(disc_real_y, disc_fake_y)\n",
    "\n",
    "        # Calculate and apply gradients\n",
    "        gen_G_gradients = tape.gradient(\n",
    "            total_gen_G_loss, self.gen_G.trainable_variables\n",
    "        )\n",
    "        gen_F_gradients = tape.gradient(\n",
    "            total_gen_F_loss, self.gen_F.trainable_variables\n",
    "        )\n",
    "        disc_X_gradients = tape.gradient(disc_X_loss, self.disc_X.trainable_variables)\n",
    "        disc_Y_gradients = tape.gradient(disc_Y_loss, self.disc_Y.trainable_variables)\n",
    "\n",
    "        # Apply gradients\n",
    "        self.gen_G_optimizer.apply_gradients(\n",
    "            zip(gen_G_gradients, self.gen_G.trainable_variables)\n",
    "        )\n",
    "        self.gen_F_optimizer.apply_gradients(\n",
    "            zip(gen_F_gradients, self.gen_F.trainable_variables)\n",
    "        )\n",
    "        self.disc_X_optimizer.apply_gradients(\n",
    "            zip(disc_X_gradients, self.disc_X.trainable_variables)\n",
    "        )\n",
    "        self.disc_Y_optimizer.apply_gradients(\n",
    "            zip(disc_Y_gradients, self.disc_Y.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # Update metrics\n",
    "        self.gen_G_loss_tracker.update_state(total_gen_G_loss)\n",
    "        self.gen_F_loss_tracker.update_state(total_gen_F_loss)\n",
    "        self.disc_X_loss_tracker.update_state(disc_X_loss)\n",
    "        self.disc_Y_loss_tracker.update_state(disc_Y_loss)\n",
    "        self.cycle_loss_tracker.update_state(cycle_loss)\n",
    "        self.identity_loss_tracker.update_state(identity_loss)\n",
    "\n",
    "        return {\n",
    "            \"gen_G_loss\": self.gen_G_loss_tracker.result(),\n",
    "            \"gen_F_loss\": self.gen_F_loss_tracker.result(),\n",
    "            \"disc_X_loss\": self.disc_X_loss_tracker.result(),\n",
    "            \"disc_Y_loss\": self.disc_Y_loss_tracker.result(),\n",
    "            \"cycle_loss\": self.cycle_loss_tracker.result(),\n",
    "            \"identity_loss\": self.identity_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def metrics(self) -> list:\n",
    "        \"\"\"\n",
    "        Returns the model's metrics.\n",
    "\n",
    "        Returns:\n",
    "            A list of metrics.\n",
    "        \"\"\"\n",
    "\n",
    "        return [\n",
    "            self.gen_G_loss_tracker,\n",
    "            self.gen_F_loss_tracker,\n",
    "            self.disc_X_loss_tracker,\n",
    "            self.disc_Y_loss_tracker,\n",
    "            self.cycle_loss_tracker,\n",
    "            self.identity_loss_tracker,\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGANEvaluator:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the CycleGAN evaluator with InceptionV3 model.\"\"\"\n",
    "        self.inception_model = tf.keras.applications.InceptionV3(\n",
    "            include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
    "        )\n",
    "\n",
    "    def preprocess_images(self, images):\n",
    "        \"\"\"Preprocess images to the correct format.\"\"\"\n",
    "        # Ensure we're working with float32\n",
    "        images = tf.cast(images, tf.float32)\n",
    "\n",
    "        # Ensure pixel values are in [0, 1]\n",
    "        if tf.reduce_max(images) > 1.0:\n",
    "            images = images / 255.0\n",
    "\n",
    "        return images\n",
    "\n",
    "    def calculate_fid(self, real_images, generated_images):\n",
    "        \"\"\"\n",
    "        Calculate Frechet Inception Distance between real and generated images.\n",
    "\n",
    "        Args:\n",
    "            real_images: Tensor of real images\n",
    "            generated_images: Tensor of generated images\n",
    "\n",
    "        Returns:\n",
    "            FID score (float)\n",
    "        \"\"\"\n",
    "        # Preprocess images\n",
    "        real_images = self.preprocess_images(real_images)\n",
    "        generated_images = self.preprocess_images(generated_images)\n",
    "\n",
    "        # Resize images to InceptionV3 input size\n",
    "        real_images = tf.image.resize(real_images, (299, 299))\n",
    "        generated_images = tf.image.resize(generated_images, (299, 299))\n",
    "\n",
    "        # Ensure batch processing for large datasets\n",
    "        batch_size = 32\n",
    "\n",
    "        def get_features(images):\n",
    "            features_list = []\n",
    "            for i in range(0, len(images), batch_size):\n",
    "                batch = images[i : i + batch_size]\n",
    "                features = self.inception_model.predict(batch, verbose=0)\n",
    "                features_list.append(features)\n",
    "            return np.concatenate(features_list, axis=0)\n",
    "\n",
    "        # Get features\n",
    "        real_features = get_features(real_images)\n",
    "        gen_features = get_features(generated_images)\n",
    "\n",
    "        # Calculate mean and covariance\n",
    "        mu1, sigma1 = np.mean(real_features, axis=0), np.cov(\n",
    "            real_features, rowvar=False\n",
    "        )\n",
    "        mu2, sigma2 = np.mean(gen_features, axis=0), np.cov(gen_features, rowvar=False)\n",
    "\n",
    "        # Calculate FID\n",
    "        ssdiff = np.sum((mu1 - mu2) ** 2.0)\n",
    "        covmean = linalg.sqrtm(sigma1.dot(sigma2), disp=False)[0]\n",
    "\n",
    "        if np.iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "\n",
    "        return float(ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean))\n",
    "\n",
    "    def calculate_ssim(self, real_images, generated_images):\n",
    "        \"\"\"Calculate SSIM between real and generated images.\"\"\"\n",
    "        real_images = self.preprocess_images(real_images)\n",
    "        generated_images = self.preprocess_images(generated_images)\n",
    "\n",
    "        ssim_scores = []\n",
    "        for real, gen in zip(real_images, generated_images):\n",
    "            # Convert to numpy and ensure correct shape\n",
    "            real_np = real.numpy()\n",
    "            gen_np = gen.numpy()\n",
    "\n",
    "            # Handle grayscale images\n",
    "            if len(real_np.shape) == 2:\n",
    "                real_np = real_np[..., np.newaxis]\n",
    "                gen_np = gen_np[..., np.newaxis]\n",
    "\n",
    "            # Use a smaller window size (5x5) and explicitly specify channel_axis\n",
    "            score = structural_similarity(\n",
    "                real_np,\n",
    "                gen_np,\n",
    "                win_size=5,  # Smaller window size\n",
    "                channel_axis=-1,  # Specify channel axis\n",
    "                data_range=1.0,\n",
    "            )\n",
    "            ssim_scores.append(score)\n",
    "\n",
    "        return float(np.mean(ssim_scores))\n",
    "\n",
    "    def calculate_mse(self, real_images, generated_images):\n",
    "        \"\"\"Calculate MSE between real and generated images.\"\"\"\n",
    "        real_images = self.preprocess_images(real_images)\n",
    "        generated_images = self.preprocess_images(generated_images)\n",
    "        return float(tf.reduce_mean(tf.square(real_images - generated_images)))\n",
    "\n",
    "    def calculate_psnr(self, real_images, generated_images):\n",
    "        \"\"\"Calculate PSNR between real and generated images.\"\"\"\n",
    "        mse = self.calculate_mse(real_images, generated_images)\n",
    "        if mse == 0:\n",
    "            return float(\"inf\")\n",
    "        return float(20 * np.log10(1.0 / tf.sqrt(mse)))\n",
    "\n",
    "    def evaluate_model(self, model, test_dataset, num_samples=100):\n",
    "        \"\"\"\n",
    "        Comprehensively evaluate the GAN model using multiple metrics.\n",
    "\n",
    "        Args:\n",
    "            model: CycleGAN model with gen_G attribute\n",
    "            test_dataset: TensorFlow dataset containing test data\n",
    "            num_samples: Number of samples to evaluate\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing evaluation metrics\n",
    "        \"\"\"\n",
    "        # Fix the dataset caching warning by proper ordering of operations\n",
    "        if num_samples is not None:\n",
    "            test_dataset = test_dataset.take(num_samples).cache()\n",
    "        else:\n",
    "            test_dataset = test_dataset.cache()\n",
    "\n",
    "        real_images = []\n",
    "        generated_images = []\n",
    "\n",
    "        try:\n",
    "            # Collect real and generated images\n",
    "            for batch in test_dataset:\n",
    "                if isinstance(batch, tuple):\n",
    "                    real_x = batch[0]\n",
    "                else:\n",
    "                    real_x = batch\n",
    "\n",
    "                # Ensure 4D shape (batch_size, height, width, channels)\n",
    "                if len(real_x.shape) != 4:\n",
    "                    raise ValueError(f\"Expected 4D tensor, got shape: {real_x.shape}\")\n",
    "\n",
    "                generated = model.gen_G(real_x, training=False)\n",
    "\n",
    "                # Ensure consistent shapes\n",
    "                if generated.shape != real_x.shape:\n",
    "                    raise ValueError(\n",
    "                        f\"Shape mismatch: real {real_x.shape} vs generated {generated.shape}\"\n",
    "                    )\n",
    "\n",
    "                real_images.append(real_x)\n",
    "                generated_images.append(generated)\n",
    "\n",
    "            # Convert to tensors\n",
    "            real_images = tf.concat(real_images, axis=0)\n",
    "            generated_images = tf.concat(generated_images, axis=0)\n",
    "\n",
    "            print(f\"Evaluating {len(real_images)} image pairs...\")\n",
    "\n",
    "            # Calculate all metrics\n",
    "            metrics = {\n",
    "                \"fid\": self.calculate_fid(real_images, generated_images),\n",
    "                \"ssim\": self.calculate_ssim(real_images, generated_images),\n",
    "                \"mse\": self.calculate_mse(real_images, generated_images),\n",
    "                \"psnr\": self.calculate_psnr(real_images, generated_images),\n",
    "            }\n",
    "\n",
    "            return metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error during evaluation:\")\n",
    "            print(f\"Original error: {str(e)}\")\n",
    "            if len(real_images) > 0:\n",
    "                print(\"\\nTensor shapes in the batch where error occurred:\")\n",
    "                print(f\"Real images shape: {real_images[-1].shape}\")\n",
    "                print(f\"Generated images shape: {generated_images[-1].shape}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsVisualizer:\n",
    "    def __init__(self, save_dir=\"./evaluation_results\"):\n",
    "        \"\"\"\n",
    "        Initialize the metrics visualizer.\n",
    "\n",
    "        Args:\n",
    "            save_dir: Directory to save the plots\n",
    "        \"\"\"\n",
    "        self.save_dir = save_dir\n",
    "        tf.io.gfile.makedirs(save_dir)\n",
    "\n",
    "    def plot_image_comparison(self, real_images, generated_images, num_samples=5):\n",
    "        \"\"\"Plot a grid of real vs generated images.\"\"\"\n",
    "        plt.figure(figsize=(15, 6))\n",
    "\n",
    "        # Randomly sample image pairs\n",
    "        total_images = len(real_images)\n",
    "        indices = np.random.choice(total_images, num_samples, replace=False)\n",
    "\n",
    "        for idx, i in enumerate(indices):\n",
    "            # Plot real image\n",
    "            plt.subplot(2, num_samples, idx + 1)\n",
    "            plt.imshow(self._prepare_image_for_plot(real_images[i]))\n",
    "            plt.axis(\"off\")\n",
    "            if idx == 0:\n",
    "                plt.title(\"Real Images\")\n",
    "\n",
    "            # Plot generated image\n",
    "            plt.subplot(2, num_samples, num_samples + idx + 1)\n",
    "            plt.imshow(self._prepare_image_for_plot(generated_images[i]))\n",
    "            plt.axis(\"off\")\n",
    "            if idx == 0:\n",
    "                plt.title(\"Generated Images\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        plt.savefig(f\"{self.save_dir}/image_comparison_{timestamp}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def plot_metrics_history(self, metrics_history):\n",
    "        \"\"\"Plot the evolution of metrics over time.\"\"\"\n",
    "        plt.figure(figsize=(15, 10))\n",
    "\n",
    "        # Create subplots for each metric\n",
    "        metrics = list(metrics_history[0].keys())\n",
    "        num_metrics = len(metrics)\n",
    "        rows = (num_metrics + 1) // 2\n",
    "\n",
    "        for idx, metric in enumerate(metrics, 1):\n",
    "            plt.subplot(rows, 2, idx)\n",
    "            values = [h[metric] for h in metrics_history]\n",
    "            epochs = range(1, len(values) + 1)\n",
    "\n",
    "            plt.plot(epochs, values, \"b-\", marker=\"o\")\n",
    "            plt.title(f\"{metric.upper()} over Time\")\n",
    "            plt.xlabel(\"Evaluation Step\")\n",
    "            plt.ylabel(metric.upper())\n",
    "            plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        plt.savefig(f\"{self.save_dir}/metrics_history_{timestamp}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def plot_metrics_distribution(self, real_images, generated_images, evaluator):\n",
    "        \"\"\"Plot the distribution of metrics across individual image pairs.\"\"\"\n",
    "        plt.figure(figsize=(15, 10))\n",
    "\n",
    "        # Calculate metrics for each image pair\n",
    "        ssim_scores = []\n",
    "        mse_scores = []\n",
    "        psnr_scores = []\n",
    "\n",
    "        for real, gen in zip(real_images, generated_images):\n",
    "            real_batch = tf.expand_dims(real, 0)\n",
    "            gen_batch = tf.expand_dims(gen, 0)\n",
    "\n",
    "            ssim = evaluator.calculate_ssim(real_batch, gen_batch)\n",
    "            mse = evaluator.calculate_mse(real_batch, gen_batch)\n",
    "            psnr = evaluator.calculate_psnr(real_batch, gen_batch)\n",
    "\n",
    "            ssim_scores.append(ssim)\n",
    "            mse_scores.append(mse)\n",
    "            psnr_scores.append(psnr)\n",
    "\n",
    "        # Plot distributions\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.hist(ssim_scores, bins=30, color=\"blue\", alpha=0.7)\n",
    "        plt.title(\"SSIM Distribution\")\n",
    "        plt.xlabel(\"SSIM Score\")\n",
    "        plt.ylabel(\"Count\")\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.hist(mse_scores, bins=30, color=\"red\", alpha=0.7)\n",
    "        plt.title(\"MSE Distribution\")\n",
    "        plt.xlabel(\"MSE Score\")\n",
    "        plt.ylabel(\"Count\")\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.hist(psnr_scores, bins=30, color=\"green\", alpha=0.7)\n",
    "        plt.title(\"PSNR Distribution\")\n",
    "        plt.xlabel(\"PSNR Score\")\n",
    "        plt.ylabel(\"Count\")\n",
    "\n",
    "        # Add summary statistics\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.axis(\"off\")\n",
    "        summary_text = (\n",
    "            f\"Summary Statistics:\\n\\n\"\n",
    "            f\"SSIM:\\n\"\n",
    "            f\"  Mean: {np.mean(ssim_scores):.4f}\\n\"\n",
    "            f\"  Std: {np.std(ssim_scores):.4f}\\n\\n\"\n",
    "            f\"MSE:\\n\"\n",
    "            f\"  Mean: {np.mean(mse_scores):.4f}\\n\"\n",
    "            f\"  Std: {np.std(mse_scores):.4f}\\n\\n\"\n",
    "            f\"PSNR:\\n\"\n",
    "            f\"  Mean: {np.mean(psnr_scores):.4f}\\n\"\n",
    "            f\"  Std: {np.std(psnr_scores):.4f}\"\n",
    "        )\n",
    "        plt.text(0.1, 0.1, summary_text, fontsize=10, fontfamily=\"monospace\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        plt.savefig(f\"{self.save_dir}/metrics_distribution_{timestamp}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def _prepare_image_for_plot(self, image):\n",
    "        \"\"\"Prepare image for plotting.\"\"\"\n",
    "        # Convert to numpy array if tensor\n",
    "        if isinstance(image, tf.Tensor):\n",
    "            image = image.numpy()\n",
    "\n",
    "        # Ensure values are in [0, 1]\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "\n",
    "        # Clip values to [0, 1]\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation_results(\n",
    "    evaluator, model, test_dataset, num_samples=100, metrics_history=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Comprehensive plotting of evaluation results.\n",
    "\n",
    "    Args:\n",
    "        evaluator: CycleGANEvaluator instance\n",
    "        model: CycleGAN model\n",
    "        test_dataset: Test dataset\n",
    "        num_samples: Number of samples to evaluate\n",
    "        metrics_history: List of metrics dictionaries over time (optional)\n",
    "    \"\"\"\n",
    "    visualizer = MetricsVisualizer()\n",
    "\n",
    "    # Collect images and calculate metrics\n",
    "    real_images = []\n",
    "    generated_images = []\n",
    "\n",
    "    # Take and cache the dataset\n",
    "    dataset = test_dataset.take(num_samples).cache()\n",
    "\n",
    "    for batch in dataset:\n",
    "        if isinstance(batch, tuple):\n",
    "            real_x = batch[0]\n",
    "        else:\n",
    "            real_x = batch\n",
    "\n",
    "        generated = model.gen_G(real_x, training=False)\n",
    "        real_images.extend(tf.unstack(real_x))\n",
    "        generated_images.extend(tf.unstack(generated))\n",
    "\n",
    "    # Convert to tensors\n",
    "    real_images = tf.stack(real_images)\n",
    "    generated_images = tf.stack(generated_images)\n",
    "\n",
    "    # Plot image comparisons\n",
    "    visualizer.plot_image_comparison(real_images, generated_images)\n",
    "\n",
    "    # Plot metrics distribution\n",
    "    visualizer.plot_metrics_distribution(real_images, generated_images, evaluator)\n",
    "\n",
    "    # Plot metrics history if provided\n",
    "    if metrics_history is not None:\n",
    "        visualizer.plot_metrics_history(metrics_history)\n",
    "\n",
    "    print(f\"Plots have been saved to {visualizer.save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGANVisualizer:\n",
    "    \"\"\"Visualization tools for CycleGAN results.\"\"\"\n",
    "\n",
    "    def __init__(self, model: \"CycleGAN\"):\n",
    "        self.model = model\n",
    "        self.utils = VisualizationUtils()\n",
    "\n",
    "    def display_samples(\n",
    "        self,\n",
    "        dataset: tf.data.Dataset,\n",
    "        num_samples: Tuple[int, int] = (4, 6),\n",
    "        figsize: Optional[Tuple[int, int]] = None,\n",
    "        save_path: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Display or save a grid of samples from the dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset: TensorFlow dataset containing the images\n",
    "            num_samples: Tuple of (rows, columns) for the display grid\n",
    "            figsize: Optional tuple for figure size\n",
    "            save_path: Optional path to save the figure\n",
    "        \"\"\"\n",
    "        rows, cols = num_samples\n",
    "        fig, axes = self.utils.create_figure(rows, cols, figsize)\n",
    "\n",
    "        samples = next(iter(dataset.take(1).batch(rows * cols)))\n",
    "\n",
    "        for idx, (ax, img) in enumerate(zip(axes, samples)):\n",
    "            if isinstance(img, tuple):\n",
    "                img = img[0]\n",
    "\n",
    "            # Display image\n",
    "            ax.imshow(self.utils.denormalize_image(img))\n",
    "            ax.axis(\"off\")\n",
    "            ax.set_title(f\"Sample {idx + 1}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "    def display_generated_samples(\n",
    "        self,\n",
    "        test_dataset: tf.data.Dataset,\n",
    "        num_samples: int = 3,\n",
    "        figsize: Optional[Tuple[int, int]] = None,\n",
    "        save_path: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Display or save pairs of input and generated images.\n",
    "\n",
    "        Args:\n",
    "            test_dataset: Dataset containing test images\n",
    "            num_samples: Number of sample pairs to display\n",
    "            figsize: Optional tuple for figure size\n",
    "            save_path: Optional path to save the figure\n",
    "        \"\"\"\n",
    "        if figsize is None:\n",
    "            figsize = (8 * 2, 4 * num_samples)\n",
    "\n",
    "        fig, axes = plt.subplots(num_samples, 2, figsize=figsize)\n",
    "        if num_samples == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "\n",
    "        for idx, samples in enumerate(test_dataset.take(num_samples)):\n",
    "            if isinstance(samples, tuple):\n",
    "                input_image = samples[0]\n",
    "            else:\n",
    "                input_image = samples\n",
    "\n",
    "            # Generate image\n",
    "            generated_image = self.model.gen_G(input_image, training=False)\n",
    "\n",
    "            axes[idx, 0].imshow(self.utils.denormalize_image(input_image[0]))\n",
    "            axes[idx, 0].axis(\"off\")\n",
    "            axes[idx, 0].set_title(\"Input Photo\")\n",
    "\n",
    "            axes[idx, 1].imshow(self.utils.denormalize_image(generated_image[0]))\n",
    "            axes[idx, 1].axis(\"off\")\n",
    "            axes[idx, 1].set_title(\"Generated Monet\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "    def display_progress(\n",
    "        self,\n",
    "        test_image: Union[str, tf.Tensor],\n",
    "        epoch: int,\n",
    "        save_dir: str = \"training_progress\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Generate and save a Monet-style image to track training progress.\n",
    "\n",
    "        Args:\n",
    "            test_image: Path to image or tensor of test image\n",
    "            epoch: Current epoch number\n",
    "            save_dir: Directory to save progress images\n",
    "        \"\"\"\n",
    "        import os\n",
    "\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        if isinstance(test_image, str):\n",
    "            img = tf.keras.preprocessing.image.load_img(\n",
    "                test_image,\n",
    "                target_size=(self.model.config.height, self.model.config.width),\n",
    "            )\n",
    "\n",
    "            img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "            img = (img / 127.5) - 1.0\n",
    "            img = tf.expand_dims(img, 0)\n",
    "\n",
    "        else:\n",
    "            img = test_image\n",
    "\n",
    "        generated = self.model.gen_G(img, training=False)\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "        ax1.imshow(self.utils.denormalize_image(img[0]))\n",
    "        ax1.axis(\"off\")\n",
    "        ax1.set_title(\"Original Photo\")\n",
    "\n",
    "        ax2.imshow(self.utils.denormalize_image(generated[0]))\n",
    "        ax2.axis(\"off\")\n",
    "        ax2.set_title(f\"Generated Monet (Epoch {epoch})\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            os.path.join(save_dir, f\"progress_epoch_{epoch}.png\"),\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=300,\n",
    "        )\n",
    "\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training(base_dir: str = \".\", batch_size: int = 1):\n",
    "    \"\"\"\n",
    "    Setups the training data and configuration.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): The base directory containing the data.\n",
    "        batch_size (int): The batch size.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the configuration, training dataset, test dataset, and steps per epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    config = ModelConfig(\n",
    "        height=256,\n",
    "        width=256,\n",
    "        channels=3,\n",
    "        base_filters=64,\n",
    "        lambda_cycle=10.0,\n",
    "        lambda_identity=0.5,\n",
    "        learning_rate=2e-4,\n",
    "        beta_1=0.5,\n",
    "    )\n",
    "\n",
    "    data_dir = Path(base_dir) / \"data\"\n",
    "    monet_files = tf.io.gfile.glob(str(data_dir / \"monet_tfrec\" / \"*.tfrec\"))\n",
    "    photo_files = tf.io.gfile.glob(str(data_dir / \"photo_tfrec\" / \"*.tfrec\"))\n",
    "\n",
    "    if not monet_files or not photo_files:\n",
    "        raise ValueError(\"No TFRecord files found in \", data_dir)\n",
    "\n",
    "    n_monet = sum(1 for f in monet_files for _ in tf.data.TFRecordDataset(f))\n",
    "    n_photo = sum(1 for f in photo_files for _ in tf.data.TFRecordDataset(f))\n",
    "    print(f\"Found {n_monet} Monet images and {n_photo} photos\")\n",
    "\n",
    "    processor = ImageProcessor(config)\n",
    "    monet_ds = processor.create_dataset(\n",
    "        monet_files, batch_size=batch_size, shuffle=True, cache=True\n",
    "    )\n",
    "\n",
    "    photo_ds = processor.create_dataset(\n",
    "        photo_files, batch_size=batch_size, shuffle=True, cache=True\n",
    "    )\n",
    "\n",
    "    test_ds = processor.create_dataset(\n",
    "        photo_files[:10],\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        cache=True,\n",
    "    )\n",
    "\n",
    "    train_ds = tf.data.Dataset.zip((photo_ds, monet_ds))\n",
    "    steps_per_epoch = min(n_monet, n_photo) // batch_size\n",
    "\n",
    "    return config, train_ds, test_ds, steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_training(config: ModelConfig, strategy):\n",
    "    \"\"\"\n",
    "    Setups the CycleGAN model and training configuration.\n",
    "\n",
    "    Args:\n",
    "        config (ModelConfig): The model configuration.\n",
    "        strategy: The distribution strategy.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the model and training callbacks.\n",
    "    \"\"\"\n",
    "\n",
    "    Path(\"logs/cyclegan\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"checkpoints/cyclegan\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with strategy.scope():\n",
    "        model = CycleGAN(config)\n",
    "        model.compile()\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=f\"logs/cyclegan/{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=\"checkpoints/cyclegan/model.{epoch:03d}.weights.h5\",\n",
    "            save_freq=\"epoch\",\n",
    "            save_weights_only=True,\n",
    "            monitor=\"gen_G_loss\",\n",
    "            mode=\"min\",\n",
    "            save_best_only=True,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    return model, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:24:39.986896: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:370] TFRecordDataset `buffer_size` is unspecified, default to 262144\n",
      "2024-11-12 15:24:39.996150: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-12 15:24:40.020250: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-12 15:24:40.060157: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-12 15:24:40.188360: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-12 15:24:40.464665: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300 Monet images and 7038 photos\n"
     ]
    }
   ],
   "source": [
    "config, train_ds, test_ds, steps_per_epoch = setup_training(\n",
    "    base_dir=\"../\", batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, callbacks = setup_model_and_training(config, strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos/miniconda3/envs/ml/lib/python3.10/site-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'cycle_gan', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1731446708.605903   16205 meta_optimizer.cc:966] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/generator_G_5/upsample_block_1/dropout_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "I0000 00:00:1731446712.213735   16420 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m295/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 352ms/step - cycle_loss: 6.7555 - disc_X_loss: 0.5806 - disc_Y_loss: 0.5975 - gen_F_loss: 7.9337 - gen_G_loss: 7.9004 - identity_loss: 0.3455"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:27:03.204112: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m299/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352ms/step - cycle_loss: 6.7436 - disc_X_loss: 0.5794 - disc_Y_loss: 0.5968 - gen_F_loss: 7.9239 - gen_G_loss: 7.8897 - identity_loss: 0.3452"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:27:04.504382: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 15:27:04.504422: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 15:27:04.504427: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 15:27:04.504430: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 15:27:04.504432: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 15:27:04.504434: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 15:27:04.504437: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 15:27:04.504439: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n",
      "/home/carlos/miniconda3/envs/ml/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 357ms/step - cycle_loss: 6.7405 - disc_X_loss: 0.5791 - disc_Y_loss: 0.5966 - gen_F_loss: 7.9214 - gen_G_loss: 7.8870 - identity_loss: 0.3451\n",
      "Epoch 2/100\n",
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 368ms/step - cycle_loss: 5.0542 - disc_X_loss: 0.3740 - disc_Y_loss: 0.5086 - gen_F_loss: 6.8324 - gen_G_loss: 6.4874 - identity_loss: 0.3014"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:28:55.653567: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 374ms/step - cycle_loss: 5.0548 - disc_X_loss: 0.3747 - disc_Y_loss: 0.5087 - gen_F_loss: 6.8319 - gen_G_loss: 6.4878 - identity_loss: 0.3015\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:28:58.859299: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 15:28:58.859347: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 15:28:58.859352: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 15:28:58.859355: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 15:28:58.859358: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 15:28:58.859360: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 15:28:58.859363: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 15:28:58.859365: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 4/100\n",
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 359ms/step - cycle_loss: 5.2722 - disc_X_loss: 0.3935 - disc_Y_loss: 0.5459 - gen_F_loss: 6.9609 - gen_G_loss: 6.6662 - identity_loss: 0.3115"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:30:50.554376: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m299/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359ms/step - cycle_loss: 5.2712 - disc_X_loss: 0.3935 - disc_Y_loss: 0.5456 - gen_F_loss: 6.9611 - gen_G_loss: 6.6656 - identity_loss: 0.3115"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:30:51.553245: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 359ms/step - cycle_loss: 5.2705 - disc_X_loss: 0.3934 - disc_Y_loss: 0.5455 - gen_F_loss: 6.9611 - gen_G_loss: 6.6650 - identity_loss: 0.3115\n",
      "Epoch 5/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:30:51.880467: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 15:30:51.880507: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 15:30:51.880511: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 15:30:51.880519: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 15:30:51.880522: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 15:30:51.880525: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 15:30:51.880527: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 15:30:51.880529: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 359ms/step - cycle_loss: 5.0111 - disc_X_loss: 0.4126 - disc_Y_loss: 0.6315 - gen_F_loss: 6.8147 - gen_G_loss: 6.2509 - identity_loss: 0.2998"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:32:38.466410: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 359ms/step - cycle_loss: 5.0120 - disc_X_loss: 0.4127 - disc_Y_loss: 0.6316 - gen_F_loss: 6.8161 - gen_G_loss: 6.2513 - identity_loss: 0.2999\n",
      "Epoch 7/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:32:39.783627: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 15:32:39.783672: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 15:32:39.783677: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 15:32:39.783679: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 15:32:39.783682: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 15:32:39.783684: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 15:32:39.783687: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 15:32:39.783689: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 358ms/step - cycle_loss: 4.7792 - disc_X_loss: 0.4512 - disc_Y_loss: 0.5964 - gen_F_loss: 6.5007 - gen_G_loss: 6.0076 - identity_loss: 0.3095"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:34:26.206056: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 358ms/step - cycle_loss: 4.7814 - disc_X_loss: 0.4520 - disc_Y_loss: 0.5962 - gen_F_loss: 6.5015 - gen_G_loss: 6.0104 - identity_loss: 0.3095\n",
      "Epoch 9/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:34:27.554131: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 15:34:27.554175: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 15:34:27.554181: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 15:34:27.554184: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 15:34:27.554186: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 15:34:27.554189: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 15:34:27.554191: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 15:34:27.554193: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 361ms/step - cycle_loss: 5.0438 - disc_X_loss: 0.5297 - disc_Y_loss: 0.5991 - gen_F_loss: 6.5358 - gen_G_loss: 6.2864 - identity_loss: 0.3086"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:36:14.681949: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 360ms/step - cycle_loss: 5.0437 - disc_X_loss: 0.5297 - disc_Y_loss: 0.5989 - gen_F_loss: 6.5356 - gen_G_loss: 6.2867 - identity_loss: 0.3086\n",
      "Epoch 11/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:36:16.031392: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 15:36:16.031435: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 15:36:16.031440: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 15:36:16.031443: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 15:36:16.031446: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 15:36:16.031448: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 15:36:16.031450: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 15:36:16.031452: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 358ms/step - cycle_loss: 4.8698 - disc_X_loss: 0.5458 - disc_Y_loss: 0.5709 - gen_F_loss: 6.3884 - gen_G_loss: 6.1572 - identity_loss: 0.3010"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:38:02.220844: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 357ms/step - cycle_loss: 4.8699 - disc_X_loss: 0.5459 - disc_Y_loss: 0.5708 - gen_F_loss: 6.3880 - gen_G_loss: 6.1577 - identity_loss: 0.3009\n",
      "Epoch 13/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:38:03.549145: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 15:38:03.549189: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 15:38:03.549194: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 15:38:03.549197: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 15:38:03.549199: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 15:38:03.549202: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 15:38:03.549204: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 15:38:03.549206: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 361ms/step - cycle_loss: 4.8874 - disc_X_loss: 0.5384 - disc_Y_loss: 0.5624 - gen_F_loss: 6.3512 - gen_G_loss: 6.1931 - identity_loss: 0.3018"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:39:50.605064: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 360ms/step - cycle_loss: 4.8868 - disc_X_loss: 0.5384 - disc_Y_loss: 0.5623 - gen_F_loss: 6.3500 - gen_G_loss: 6.1930 - identity_loss: 0.3018\n",
      "Epoch 15/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:39:51.925536: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 15:39:51.925578: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 15:39:51.925583: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 15:39:51.925586: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 15:39:51.925589: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 15:39:51.925592: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 15:39:51.925594: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 15:39:51.925597: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 358ms/step - cycle_loss: 4.7917 - disc_X_loss: 0.5755 - disc_Y_loss: 0.5172 - gen_F_loss: 6.1659 - gen_G_loss: 6.1832 - identity_loss: 0.2950"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:41:38.327926: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 358ms/step - cycle_loss: 4.7916 - disc_X_loss: 0.5750 - disc_Y_loss: 0.5176 - gen_F_loss: 6.1667 - gen_G_loss: 6.1828 - identity_loss: 0.2950\n",
      "Epoch 17/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:41:39.635573: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 15:41:39.635619: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 15:41:39.635624: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 15:41:39.635627: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 15:41:39.635631: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 15:41:39.635633: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 15:41:39.635635: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 15:41:39.635638: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 358ms/step - cycle_loss: 4.6668 - disc_X_loss: 0.5698 - disc_Y_loss: 0.5097 - gen_F_loss: 6.0293 - gen_G_loss: 6.0747 - identity_loss: 0.2992"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:43:25.946176: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 358ms/step - cycle_loss: 4.6665 - disc_X_loss: 0.5699 - disc_Y_loss: 0.5099 - gen_F_loss: 6.0283 - gen_G_loss: 6.0743 - identity_loss: 0.2992\n",
      "Epoch 19/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:43:27.312045: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 15:43:27.312094: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 15:43:27.312099: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 15:43:27.312101: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 15:43:27.312104: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 15:43:27.312106: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 15:43:27.312109: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 15:43:27.312111: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 360ms/step - cycle_loss: 4.6070 - disc_X_loss: 0.5731 - disc_Y_loss: 0.5317 - gen_F_loss: 5.9390 - gen_G_loss: 5.9828 - identity_loss: 0.2907"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:45:14.341632: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 360ms/step - cycle_loss: 4.6071 - disc_X_loss: 0.5730 - disc_Y_loss: 0.5318 - gen_F_loss: 5.9388 - gen_G_loss: 5.9831 - identity_loss: 0.2907\n",
      "Epoch 21/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:45:15.677949: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 15:45:15.678001: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 15:45:15.678008: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 15:45:15.678011: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 15:45:15.678016: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 15:45:15.678020: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 15:45:15.678022: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 15:45:15.678024: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m297/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 368ms/step - cycle_loss: 4.4608 - disc_X_loss: 0.5510 - disc_Y_loss: 0.5449 - gen_F_loss: 5.8067 - gen_G_loss: 5.7920 - identity_loss: 0.2916"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:47:05.239957: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 368ms/step - cycle_loss: 4.4605 - disc_X_loss: 0.5511 - disc_Y_loss: 0.5448 - gen_F_loss: 5.8065 - gen_G_loss: 5.7923 - identity_loss: 0.2916\n",
      "Epoch 23/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:47:06.316307: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 15:47:06.316346: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 15:47:06.316351: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 15:47:06.316354: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 15:47:06.316357: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 15:47:06.316359: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 15:47:06.316361: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n",
      "2024-11-12 15:47:06.316364: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 11543672725576321583\n",
      "2024-11-12 15:47:06.316366: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 359ms/step - cycle_loss: 4.2991 - disc_X_loss: 0.5860 - disc_Y_loss: 0.5383 - gen_F_loss: 5.6110 - gen_G_loss: 5.6327 - identity_loss: 0.2900"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:48:52.808715: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 358ms/step - cycle_loss: 4.2981 - disc_X_loss: 0.5856 - disc_Y_loss: 0.5385 - gen_F_loss: 5.6102 - gen_G_loss: 5.6317 - identity_loss: 0.2899\n",
      "Epoch 25/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:48:54.193673: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{function_node cond_false_11538}}{{node cond/IteratorGetNext}}]]\n",
      "2024-11-12 15:48:54.193729: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 15:48:54.193734: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 15:48:54.193737: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 15:48:54.193739: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 15:48:54.193742: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 15:48:54.193744: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 15:48:54.193747: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 15:48:54.193749: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 359ms/step - cycle_loss: 4.4952 - disc_X_loss: 0.5107 - disc_Y_loss: 0.5459 - gen_F_loss: 5.9613 - gen_G_loss: 5.8248 - identity_loss: 0.2913"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:50:40.692158: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 368ms/step - cycle_loss: 4.4929 - disc_X_loss: 0.5111 - disc_Y_loss: 0.5463 - gen_F_loss: 5.9582 - gen_G_loss: 5.8223 - identity_loss: 0.2913\n",
      "Epoch 27/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:50:44.971041: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 15:50:44.971097: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 15:50:44.971105: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 15:50:44.971109: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 15:50:44.971114: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 15:50:44.971118: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 15:50:44.971122: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 15:50:44.971125: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 359ms/step - cycle_loss: 4.1664 - disc_X_loss: 0.5993 - disc_Y_loss: 0.5675 - gen_F_loss: 5.4027 - gen_G_loss: 5.4576 - identity_loss: 0.2831"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:52:31.556496: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 359ms/step - cycle_loss: 4.1654 - disc_X_loss: 0.5991 - disc_Y_loss: 0.5675 - gen_F_loss: 5.4021 - gen_G_loss: 5.4567 - identity_loss: 0.2830\n",
      "Epoch 29/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:52:32.916963: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 15:52:32.917009: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 15:52:32.917014: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 15:52:32.917018: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 15:52:32.917021: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 15:52:32.917023: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 15:52:32.917026: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 15:52:32.917028: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 360ms/step - cycle_loss: 4.0479 - disc_X_loss: 0.5355 - disc_Y_loss: 0.5646 - gen_F_loss: 5.4208 - gen_G_loss: 5.3376 - identity_loss: 0.2757"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:54:19.773680: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 360ms/step - cycle_loss: 4.0481 - disc_X_loss: 0.5358 - disc_Y_loss: 0.5647 - gen_F_loss: 5.4205 - gen_G_loss: 5.3377 - identity_loss: 0.2757\n",
      "Epoch 31/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:54:21.101934: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 15:54:21.101986: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 15:54:21.101990: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 15:54:21.101993: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 15:54:21.101996: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 15:54:21.101998: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 15:54:21.102001: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 15:54:21.102003: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 357ms/step - cycle_loss: 3.8744 - disc_X_loss: 0.5607 - disc_Y_loss: 0.5724 - gen_F_loss: 5.1995 - gen_G_loss: 5.1227 - identity_loss: 0.2794"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:56:07.230899: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 357ms/step - cycle_loss: 3.8754 - disc_X_loss: 0.5606 - disc_Y_loss: 0.5725 - gen_F_loss: 5.2004 - gen_G_loss: 5.1238 - identity_loss: 0.2794\n",
      "Epoch 33/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:56:08.604909: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 15:56:08.604958: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 15:56:08.604964: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 15:56:08.604967: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 15:56:08.604971: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 15:56:08.604973: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 15:56:08.604976: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 15:56:08.604978: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 357ms/step - cycle_loss: 3.9917 - disc_X_loss: 0.5567 - disc_Y_loss: 0.5727 - gen_F_loss: 5.3404 - gen_G_loss: 5.2513 - identity_loss: 0.2749"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:57:54.662072: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 357ms/step - cycle_loss: 3.9911 - disc_X_loss: 0.5564 - disc_Y_loss: 0.5727 - gen_F_loss: 5.3402 - gen_G_loss: 5.2510 - identity_loss: 0.2749\n",
      "Epoch 35/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:57:55.990700: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 15:57:55.990746: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 15:57:55.990751: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 15:57:55.990754: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 15:57:55.990757: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 15:57:55.990759: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 15:57:55.990761: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 15:57:55.990764: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 360ms/step - cycle_loss: 3.8968 - disc_X_loss: 0.5689 - disc_Y_loss: 0.5672 - gen_F_loss: 5.2542 - gen_G_loss: 5.2093 - identity_loss: 0.2867"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:59:42.862203: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 360ms/step - cycle_loss: 3.8975 - disc_X_loss: 0.5686 - disc_Y_loss: 0.5674 - gen_F_loss: 5.2549 - gen_G_loss: 5.2096 - identity_loss: 0.2867\n",
      "Epoch 37/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:59:44.204673: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 15:59:44.204721: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 15:59:44.204727: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 15:59:44.204729: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 15:59:44.204732: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 15:59:44.204734: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 15:59:44.204737: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 15:59:44.204739: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 354ms/step - cycle_loss: 3.7855 - disc_X_loss: 0.5857 - disc_Y_loss: 0.5719 - gen_F_loss: 5.1084 - gen_G_loss: 5.0685 - identity_loss: 0.2811"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:01:29.353804: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 354ms/step - cycle_loss: 3.7865 - disc_X_loss: 0.5852 - disc_Y_loss: 0.5719 - gen_F_loss: 5.1097 - gen_G_loss: 5.0698 - identity_loss: 0.2810\n",
      "Epoch 39/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:01:30.682660: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:01:30.682702: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:01:30.682708: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:01:30.682710: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:01:30.682713: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:01:30.682715: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:01:30.682717: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:01:30.682720: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 355ms/step - cycle_loss: 3.9434 - disc_X_loss: 0.5368 - disc_Y_loss: 0.5878 - gen_F_loss: 5.2984 - gen_G_loss: 5.1964 - identity_loss: 0.2773"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:03:16.074874: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 354ms/step - cycle_loss: 3.9423 - disc_X_loss: 0.5368 - disc_Y_loss: 0.5879 - gen_F_loss: 5.2974 - gen_G_loss: 5.1952 - identity_loss: 0.2773\n",
      "Epoch 41/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:03:17.351905: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:03:17.351971: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:03:17.351991: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:03:17.351995: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:03:17.352000: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:03:17.352004: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:03:17.352006: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:03:17.352008: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 356ms/step - cycle_loss: 3.6730 - disc_X_loss: 0.5536 - disc_Y_loss: 0.5643 - gen_F_loss: 5.0137 - gen_G_loss: 4.9559 - identity_loss: 0.2719"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:05:03.068315: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 356ms/step - cycle_loss: 3.6735 - disc_X_loss: 0.5535 - disc_Y_loss: 0.5646 - gen_F_loss: 5.0146 - gen_G_loss: 4.9560 - identity_loss: 0.2720\n",
      "Epoch 43/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:05:04.373334: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:05:04.373376: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:05:04.373381: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:05:04.373384: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:05:04.373387: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:05:04.373389: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:05:04.373392: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:05:04.373394: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 355ms/step - cycle_loss: 3.7634 - disc_X_loss: 0.5275 - disc_Y_loss: 0.5872 - gen_F_loss: 5.1623 - gen_G_loss: 5.0401 - identity_loss: 0.2813"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:06:49.845763: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 355ms/step - cycle_loss: 3.7639 - disc_X_loss: 0.5273 - disc_Y_loss: 0.5872 - gen_F_loss: 5.1634 - gen_G_loss: 5.0407 - identity_loss: 0.2813\n",
      "Epoch 45/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:06:51.131648: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:06:51.131694: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:06:51.131699: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:06:51.131702: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:06:51.131705: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:06:51.131707: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:06:51.131710: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:06:51.131712: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 352ms/step - cycle_loss: 3.7995 - disc_X_loss: 0.5326 - disc_Y_loss: 0.5783 - gen_F_loss: 5.2584 - gen_G_loss: 5.0479 - identity_loss: 0.2802"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:08:35.552865: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 351ms/step - cycle_loss: 3.7983 - disc_X_loss: 0.5328 - disc_Y_loss: 0.5783 - gen_F_loss: 5.2565 - gen_G_loss: 5.0470 - identity_loss: 0.2801\n",
      "Epoch 47/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:08:36.908093: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:08:36.908137: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:08:36.908143: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:08:36.908145: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:08:36.908148: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:08:36.908150: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:08:36.908153: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:08:36.908156: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 353ms/step - cycle_loss: 3.6380 - disc_X_loss: 0.5395 - disc_Y_loss: 0.5564 - gen_F_loss: 4.9993 - gen_G_loss: 4.9485 - identity_loss: 0.2698"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:10:21.707541: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 352ms/step - cycle_loss: 3.6376 - disc_X_loss: 0.5392 - disc_Y_loss: 0.5567 - gen_F_loss: 4.9993 - gen_G_loss: 4.9478 - identity_loss: 0.2698\n",
      "Epoch 49/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:10:22.995213: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:10:22.995260: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:10:22.995265: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:10:22.995268: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:10:22.995271: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:10:22.995274: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:10:22.995277: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:10:22.995280: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 354ms/step - cycle_loss: 3.5243 - disc_X_loss: 0.5335 - disc_Y_loss: 0.5539 - gen_F_loss: 4.9524 - gen_G_loss: 4.8367 - identity_loss: 0.2777"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:12:08.011783: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 353ms/step - cycle_loss: 3.5251 - disc_X_loss: 0.5335 - disc_Y_loss: 0.5540 - gen_F_loss: 4.9534 - gen_G_loss: 4.8377 - identity_loss: 0.2776\n",
      "Epoch 51/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:12:09.315774: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:12:09.315824: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:12:09.315830: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:12:09.315833: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:12:09.315837: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:12:09.315839: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:12:09.315842: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:12:09.315844: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 355ms/step - cycle_loss: 3.5504 - disc_X_loss: 0.5204 - disc_Y_loss: 0.5625 - gen_F_loss: 5.0052 - gen_G_loss: 4.8605 - identity_loss: 0.2796"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:13:54.820230: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 355ms/step - cycle_loss: 3.5504 - disc_X_loss: 0.5205 - disc_Y_loss: 0.5625 - gen_F_loss: 5.0053 - gen_G_loss: 4.8608 - identity_loss: 0.2796\n",
      "Epoch 53/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:13:56.122629: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:13:56.122674: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:13:56.122680: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:13:56.122682: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:13:56.122686: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:13:56.122688: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:13:56.122690: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:13:56.122693: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m297/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 354ms/step - cycle_loss: 3.5955 - disc_X_loss: 0.4833 - disc_Y_loss: 0.5948 - gen_F_loss: 5.0787 - gen_G_loss: 4.8610 - identity_loss: 0.2765"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:15:41.485943: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 354ms/step - cycle_loss: 3.5945 - disc_X_loss: 0.4836 - disc_Y_loss: 0.5949 - gen_F_loss: 5.0775 - gen_G_loss: 4.8601 - identity_loss: 0.2765\n",
      "Epoch 55/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:15:42.529531: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:15:42.529579: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:15:42.529585: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:15:42.529587: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:15:42.529590: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:15:42.529593: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:15:42.529595: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:15:42.529597: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m297/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 354ms/step - cycle_loss: 3.5504 - disc_X_loss: 0.5460 - disc_Y_loss: 0.5728 - gen_F_loss: 4.9161 - gen_G_loss: 4.8135 - identity_loss: 0.2700"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:17:27.901463: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 353ms/step - cycle_loss: 3.5509 - disc_X_loss: 0.5461 - disc_Y_loss: 0.5729 - gen_F_loss: 4.9170 - gen_G_loss: 4.8144 - identity_loss: 0.2701\n",
      "Epoch 57/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:17:28.875060: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:17:28.875107: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:17:28.875112: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:17:28.875115: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:17:28.875118: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:17:28.875120: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:17:28.875123: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:17:28.875128: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 354ms/step - cycle_loss: 3.4251 - disc_X_loss: 0.5572 - disc_Y_loss: 0.5454 - gen_F_loss: 4.7627 - gen_G_loss: 4.7858 - identity_loss: 0.2726"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:19:13.918682: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 353ms/step - cycle_loss: 3.4259 - disc_X_loss: 0.5569 - disc_Y_loss: 0.5459 - gen_F_loss: 4.7638 - gen_G_loss: 4.7860 - identity_loss: 0.2726\n",
      "Epoch 59/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:19:15.243421: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:19:15.243471: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:19:15.243475: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:19:15.243478: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:19:15.243482: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:19:15.243484: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:19:15.243486: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:19:15.243489: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 354ms/step - cycle_loss: 3.4603 - disc_X_loss: 0.5346 - disc_Y_loss: 0.5402 - gen_F_loss: 4.8634 - gen_G_loss: 4.7927 - identity_loss: 0.2778"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:21:00.316254: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 353ms/step - cycle_loss: 3.4605 - disc_X_loss: 0.5347 - disc_Y_loss: 0.5405 - gen_F_loss: 4.8637 - gen_G_loss: 4.7930 - identity_loss: 0.2777\n",
      "Epoch 61/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:21:01.617674: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:21:01.617714: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:21:01.617722: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:21:01.617724: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:21:01.617727: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:21:01.617729: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:21:01.617731: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:21:01.617734: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 359ms/step - cycle_loss: 3.4363 - disc_X_loss: 0.5348 - disc_Y_loss: 0.5365 - gen_F_loss: 4.8294 - gen_G_loss: 4.7695 - identity_loss: 0.2706"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:22:48.142920: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 358ms/step - cycle_loss: 3.4359 - disc_X_loss: 0.5347 - disc_Y_loss: 0.5368 - gen_F_loss: 4.8296 - gen_G_loss: 4.7690 - identity_loss: 0.2706\n",
      "Epoch 63/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:22:49.486430: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:22:49.486475: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:22:49.486480: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:22:49.486482: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:22:49.486485: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:22:49.486487: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:22:49.486490: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:22:49.486492: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395ms/step - cycle_loss: 3.4283 - disc_X_loss: 0.5841 - disc_Y_loss: 0.5697 - gen_F_loss: 4.7394 - gen_G_loss: 4.7523 - identity_loss: 0.2696"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:24:46.855416: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 395ms/step - cycle_loss: 3.4280 - disc_X_loss: 0.5836 - disc_Y_loss: 0.5698 - gen_F_loss: 4.7400 - gen_G_loss: 4.7520 - identity_loss: 0.2696\n",
      "Epoch 65/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:24:48.408491: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:24:48.408535: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:24:48.408540: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:24:48.408543: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:24:48.408547: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:24:48.408549: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:24:48.408551: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:24:48.408554: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391ms/step - cycle_loss: 3.4277 - disc_X_loss: 0.5454 - disc_Y_loss: 0.5563 - gen_F_loss: 4.7897 - gen_G_loss: 4.7295 - identity_loss: 0.2657"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:26:44.643734: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 391ms/step - cycle_loss: 3.4268 - disc_X_loss: 0.5453 - disc_Y_loss: 0.5563 - gen_F_loss: 4.7891 - gen_G_loss: 4.7292 - identity_loss: 0.2657\n",
      "Epoch 67/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:26:46.087222: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:26:46.087269: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:26:46.087277: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:26:46.087280: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:26:46.087284: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:26:46.087288: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:26:46.087291: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:26:46.087295: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 400ms/step - cycle_loss: 3.3904 - disc_X_loss: 0.5283 - disc_Y_loss: 0.5351 - gen_F_loss: 4.7994 - gen_G_loss: 4.7427 - identity_loss: 0.2653"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:28:44.756760: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m299/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - cycle_loss: 3.3900 - disc_X_loss: 0.5284 - disc_Y_loss: 0.5354 - gen_F_loss: 4.7988 - gen_G_loss: 4.7419 - identity_loss: 0.2653"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:28:45.840832: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 399ms/step - cycle_loss: 3.3897 - disc_X_loss: 0.5284 - disc_Y_loss: 0.5356 - gen_F_loss: 4.7984 - gen_G_loss: 4.7414 - identity_loss: 0.2652\n",
      "Epoch 69/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:28:46.240707: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:28:46.240765: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:28:46.240774: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:28:46.240779: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:28:46.240784: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:28:46.240788: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:28:46.240793: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:28:46.240797: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394ms/step - cycle_loss: 3.3207 - disc_X_loss: 0.5567 - disc_Y_loss: 0.5616 - gen_F_loss: 4.6697 - gen_G_loss: 4.6346 - identity_loss: 0.2612"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:30:43.134197: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 393ms/step - cycle_loss: 3.3202 - disc_X_loss: 0.5564 - disc_Y_loss: 0.5618 - gen_F_loss: 4.6697 - gen_G_loss: 4.6341 - identity_loss: 0.2612\n",
      "Epoch 71/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:30:44.547118: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:30:44.547159: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:30:44.547165: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:30:44.547168: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:30:44.547171: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:30:44.547173: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:30:44.547176: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:30:44.547178: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m297/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 383ms/step - cycle_loss: 3.2641 - disc_X_loss: 0.5416 - disc_Y_loss: 0.5689 - gen_F_loss: 4.6272 - gen_G_loss: 4.5283 - identity_loss: 0.2529"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:32:38.745922: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 383ms/step - cycle_loss: 3.2643 - disc_X_loss: 0.5414 - disc_Y_loss: 0.5690 - gen_F_loss: 4.6280 - gen_G_loss: 4.5289 - identity_loss: 0.2530\n",
      "Epoch 73/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:32:39.862055: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:32:39.862108: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:32:39.862114: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:32:39.862117: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:32:39.862121: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:32:39.862123: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:32:39.862126: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:32:39.862129: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 383ms/step - cycle_loss: 3.2957 - disc_X_loss: 0.5067 - disc_Y_loss: 0.5364 - gen_F_loss: 4.7336 - gen_G_loss: 4.6306 - identity_loss: 0.2672"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:34:33.618019: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 383ms/step - cycle_loss: 3.2947 - disc_X_loss: 0.5070 - disc_Y_loss: 0.5367 - gen_F_loss: 4.7324 - gen_G_loss: 4.6297 - identity_loss: 0.2672\n",
      "Epoch 75/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:34:35.072171: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:34:35.072247: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:34:35.072252: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:34:35.072255: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:34:35.072258: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:34:35.072261: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:34:35.072263: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:34:35.072265: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392ms/step - cycle_loss: 3.2841 - disc_X_loss: 0.5513 - disc_Y_loss: 0.5485 - gen_F_loss: 4.6994 - gen_G_loss: 4.6825 - identity_loss: 0.2706"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:36:31.466693: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 392ms/step - cycle_loss: 3.2832 - disc_X_loss: 0.5512 - disc_Y_loss: 0.5488 - gen_F_loss: 4.6980 - gen_G_loss: 4.6808 - identity_loss: 0.2705\n",
      "Epoch 77/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:36:32.908279: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:36:32.908336: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:36:32.908342: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:36:32.908344: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:36:32.908347: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:36:32.908350: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:36:32.908352: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:36:32.908355: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 386ms/step - cycle_loss: 3.2077 - disc_X_loss: 0.5539 - disc_Y_loss: 0.5445 - gen_F_loss: 4.5631 - gen_G_loss: 4.5635 - identity_loss: 0.2673"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:38:27.621793: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 386ms/step - cycle_loss: 3.2078 - disc_X_loss: 0.5538 - disc_Y_loss: 0.5449 - gen_F_loss: 4.5631 - gen_G_loss: 4.5630 - identity_loss: 0.2672\n",
      "Epoch 79/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:38:29.084668: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:38:29.084729: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:38:29.084734: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:38:29.084736: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:38:29.084740: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:38:29.084743: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:38:29.084749: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:38:29.084755: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391ms/step - cycle_loss: 3.1194 - disc_X_loss: 0.5459 - disc_Y_loss: 0.5388 - gen_F_loss: 4.5053 - gen_G_loss: 4.4756 - identity_loss: 0.2657"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:40:25.109758: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 391ms/step - cycle_loss: 3.1204 - disc_X_loss: 0.5457 - disc_Y_loss: 0.5393 - gen_F_loss: 4.5062 - gen_G_loss: 4.4759 - identity_loss: 0.2656\n",
      "Epoch 81/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:40:26.593818: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:40:26.593885: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:40:26.593895: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:40:26.593899: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:40:26.593904: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:40:26.593908: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:40:26.593912: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:40:26.593915: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m297/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397ms/step - cycle_loss: 3.1678 - disc_X_loss: 0.5355 - disc_Y_loss: 0.5389 - gen_F_loss: 4.5870 - gen_G_loss: 4.5487 - identity_loss: 0.2692"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:42:24.858608: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 397ms/step - cycle_loss: 3.1673 - disc_X_loss: 0.5355 - disc_Y_loss: 0.5394 - gen_F_loss: 4.5862 - gen_G_loss: 4.5470 - identity_loss: 0.2691\n",
      "Epoch 83/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:42:25.933044: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:42:25.933120: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:42:25.933127: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:42:25.933131: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:42:25.933135: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:42:25.933139: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:42:25.933142: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:42:25.933145: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 393ms/step - cycle_loss: 2.9699 - disc_X_loss: 0.5498 - disc_Y_loss: 0.5418 - gen_F_loss: 4.3316 - gen_G_loss: 4.2879 - identity_loss: 0.2632"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:44:22.727289: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 394ms/step - cycle_loss: 2.9704 - disc_X_loss: 0.5497 - disc_Y_loss: 0.5420 - gen_F_loss: 4.3320 - gen_G_loss: 4.2887 - identity_loss: 0.2631\n",
      "Epoch 85/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:44:24.389419: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:44:24.389470: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:44:24.389480: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:44:24.389483: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:44:24.389486: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:44:24.389489: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:44:24.389492: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:44:24.389495: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 381ms/step - cycle_loss: 3.1490 - disc_X_loss: 0.5282 - disc_Y_loss: 0.5313 - gen_F_loss: 4.5818 - gen_G_loss: 4.5447 - identity_loss: 0.2708"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:46:17.881050: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 381ms/step - cycle_loss: 3.1484 - disc_X_loss: 0.5282 - disc_Y_loss: 0.5318 - gen_F_loss: 4.5809 - gen_G_loss: 4.5432 - identity_loss: 0.2707\n",
      "Epoch 87/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:46:19.262071: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:46:19.262122: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:46:19.262128: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:46:19.262131: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:46:19.262134: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:46:19.262136: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:46:19.262139: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:46:19.262141: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 410ms/step - cycle_loss: 3.1616 - disc_X_loss: 0.5456 - disc_Y_loss: 0.5392 - gen_F_loss: 4.5612 - gen_G_loss: 4.5367 - identity_loss: 0.2641"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:48:20.988443: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 410ms/step - cycle_loss: 3.1603 - disc_X_loss: 0.5457 - disc_Y_loss: 0.5396 - gen_F_loss: 4.5594 - gen_G_loss: 4.5347 - identity_loss: 0.2641\n",
      "Epoch 89/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:48:22.634053: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:48:22.634130: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:48:22.634139: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:48:22.634144: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:48:22.634150: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:48:22.634154: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:48:22.634159: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:48:22.634164: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 389ms/step - cycle_loss: 3.0430 - disc_X_loss: 0.5623 - disc_Y_loss: 0.5593 - gen_F_loss: 4.3559 - gen_G_loss: 4.3558 - identity_loss: 0.2518"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:50:18.331860: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 388ms/step - cycle_loss: 3.0426 - disc_X_loss: 0.5624 - disc_Y_loss: 0.5594 - gen_F_loss: 4.3556 - gen_G_loss: 4.3553 - identity_loss: 0.2518\n",
      "Epoch 91/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:50:19.685735: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:50:19.685778: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:50:19.685784: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:50:19.685787: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:50:19.685790: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:50:19.685792: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:50:19.685795: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:50:19.685797: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374ms/step - cycle_loss: 3.0475 - disc_X_loss: 0.5575 - disc_Y_loss: 0.5372 - gen_F_loss: 4.3724 - gen_G_loss: 4.4063 - identity_loss: 0.2545"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:52:10.710909: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 374ms/step - cycle_loss: 3.0484 - disc_X_loss: 0.5573 - disc_Y_loss: 0.5372 - gen_F_loss: 4.3734 - gen_G_loss: 4.4075 - identity_loss: 0.2546\n",
      "Epoch 93/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:52:12.156065: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:52:12.156119: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:52:12.156126: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:52:12.156130: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:52:12.156135: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:52:12.156139: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:52:12.156143: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:52:12.156147: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 375ms/step - cycle_loss: 3.0998 - disc_X_loss: 0.5730 - disc_Y_loss: 0.5561 - gen_F_loss: 4.4239 - gen_G_loss: 4.4294 - identity_loss: 0.2614"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:54:03.537742: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 375ms/step - cycle_loss: 3.0985 - disc_X_loss: 0.5729 - disc_Y_loss: 0.5565 - gen_F_loss: 4.4227 - gen_G_loss: 4.4274 - identity_loss: 0.2615\n",
      "Epoch 95/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:54:04.925385: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:54:04.925415: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:54:04.925418: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:54:04.925421: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:54:04.925423: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:54:04.925426: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:54:04.925429: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n",
      "2024-11-12 16:54:04.925431: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372ms/step - cycle_loss: 2.9488 - disc_X_loss: 0.5827 - disc_Y_loss: 0.5536 - gen_F_loss: 4.2157 - gen_G_loss: 4.2745 - identity_loss: 0.2607"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:55:55.237513: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 371ms/step - cycle_loss: 2.9484 - disc_X_loss: 0.5824 - disc_Y_loss: 0.5539 - gen_F_loss: 4.2155 - gen_G_loss: 4.2735 - identity_loss: 0.2606\n",
      "Epoch 97/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:55:56.588773: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:55:56.588816: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:55:56.588822: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:55:56.588824: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:55:56.588827: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:55:56.588829: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:55:56.588832: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:55:56.588834: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 384ms/step - cycle_loss: 2.9067 - disc_X_loss: 0.5698 - disc_Y_loss: 0.5503 - gen_F_loss: 4.2224 - gen_G_loss: 4.2867 - identity_loss: 0.2650"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:57:50.652607: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 384ms/step - cycle_loss: 2.9067 - disc_X_loss: 0.5696 - disc_Y_loss: 0.5506 - gen_F_loss: 4.2224 - gen_G_loss: 4.2855 - identity_loss: 0.2649\n",
      "Epoch 99/100\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73us/step - cycle_loss: 0.0000e+00 - disc_X_loss: 0.0000e+00 - disc_Y_loss: 0.0000e+00 - gen_F_loss: 0.0000e+00 - gen_G_loss: 0.0000e+00 - identity_loss: 0.0000e+00\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:57:52.039891: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 605559996236928040\n",
      "2024-11-12 16:57:52.039928: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 12617876961775868396\n",
      "2024-11-12 16:57:52.039933: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 13602789619624848076\n",
      "2024-11-12 16:57:52.039935: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 3365725558163435500\n",
      "2024-11-12 16:57:52.039938: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 10207714368467501151\n",
      "2024-11-12 16:57:52.039941: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 7163868573839600055\n",
      "2024-11-12 16:57:52.039943: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 5661041136170547077\n",
      "2024-11-12 16:57:52.039945: I tensorflow/core/framework/local_rendezvous.cc:428] Local rendezvous send item cancelled. Key hash: 15906979025608808287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m296/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 375ms/step - cycle_loss: 2.9169 - disc_X_loss: 0.5635 - disc_Y_loss: 0.5295 - gen_F_loss: 4.2578 - gen_G_loss: 4.2747 - identity_loss: 0.2599"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:59:43.244152: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 374ms/step - cycle_loss: 2.9176 - disc_X_loss: 0.5631 - disc_Y_loss: 0.5301 - gen_F_loss: 4.2589 - gen_G_loss: 4.2749 - identity_loss: 0.2599\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = CycleGANVisualizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:59:45.012704: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "visualizer.display_generated_samples(\n",
    "    test_ds, num_samples=8, save_path=\"generated_samples.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = CycleGANEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:59:52.767006: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 100 image pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731452393.962408   16418 service.cc:148] XLA service 0x55a94149b130 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1731452393.962456   16418 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2024-11-12 16:59:54.049913: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-12 16:59:56.950573: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 438.21MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-11-12 16:59:57.752440: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 438.29MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-11-12 16:59:57.752504: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 593.83MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "I0000 00:00:1731452407.211964   16418 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-11-12 17:00:31.725337: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots have been saved to ./evaluation_results\n",
      "Fid: 166.6465\n",
      "Ssim: 0.2779\n",
      "Mse: 0.1552\n",
      "Psnr: 8.0915\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    metrics = evaluator.evaluate_model(model, test_ds, num_samples=100)\n",
    "    metrics_history.append(metrics)\n",
    "\n",
    "    plot_evaluation_results(\n",
    "        evaluator=evaluator,\n",
    "        model=model,\n",
    "        test_dataset=test_ds,\n",
    "        num_samples=100,\n",
    "        metrics_history=metrics_history,\n",
    "    )\n",
    "\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name.capitalize()}: {value:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"cyclegan_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"cyclegan_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "generator_g = model.gen_G\n",
    "generator_g.save(\"monet_generator.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: generator_saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: generator_saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "@tf.function(\n",
    "    input_signature=[tf.TensorSpec(shape=[None, 256, 256, 3], dtype=tf.float32)]\n",
    ")\n",
    "def serve(input_img):\n",
    "    return generator_g(input_img)\n",
    "\n",
    "\n",
    "# Save with signatures\n",
    "tf.saved_model.save(\n",
    "    generator_g, \"generator_saved_model\", signatures={\"serving_default\": serve}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
